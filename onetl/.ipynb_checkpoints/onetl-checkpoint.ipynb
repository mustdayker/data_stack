{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecaf169",
   "metadata": {},
   "source": [
    "<a id='head'></a>\n",
    "# onETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deaa3d8",
   "metadata": {},
   "source": [
    "# Оглавление\n",
    "\n",
    "- [**1. Введение в onETL**](#part_01)\n",
    "    - [1.1 Описание базовых функций](#part_11)\n",
    "    - [1.2 Настройка](#part_12)\n",
    "    - [1.3 Установка](#part_13)\n",
    "    - [1.4 Архитектура Spark](#part_14)\n",
    "    - [1.5 Запуск сессии Spark](#part_15)\n",
    "    \n",
    "- [**2. Коннекторы к БД**](#part_02)\n",
    "    - [2.1 Введение](#part_21)\n",
    "    - [2.2 Логирование](#part_22)\n",
    "    - [2.3 DB Connections](#part_23)\n",
    "    - [2.4 Методы коннекторов](#part_24)\n",
    "    - [2.5 Read and Write Options](#part_25)\n",
    "    - [2.6 Демо](#part_26)\n",
    "    \n",
    "- [**3. Объекты манипуляции данными DBReader и DBWriter**](#part_03)\n",
    "    - [3.1 DBReader()](#part_31)\n",
    "    - [3.2 DBWriter()](#part_32)\n",
    "    - [3.3 Демо](#part_33)\n",
    "\n",
    "- [**4. Объекты подключения и манипуляции данными с использованием структуры DataFrame**](#part_04)\n",
    "    - [4.1 FileDataFrameConnections](#part_41)\n",
    "    - [4.2 LocalFS](#part_42)\n",
    "    - [4.3 HDFS](#part_43)\n",
    "    - [4.4 S3](#part_44)\n",
    "    - [4.5 Files Formats](#part_45)\n",
    "    - [4.6 FileDFReader](#part_46)\n",
    "    - [4.7 FileDFWriter](#part_47)\n",
    "    - [4.8 Демо](#part_48)\n",
    "\n",
    "- [**5. Нестандартные коннекторы: Greenplum, MongoDB, Teradata**](#part_05)\n",
    "    - [5.1 Введение](#part_51)\n",
    "    - [5.2 Hive](#part_52)\n",
    "    - [5.3 Greenplum](#part_53)\n",
    "    - [5.4 Clickhouse](#part_54)\n",
    "    - [5.5 Kafka](#part_55)\n",
    "    - [5.6 MongoDB](#part_56)\n",
    "    - [5.7 Teradata](#part_57)\n",
    "    - [5.8 Демо](#part_58)\n",
    "    \n",
    "- [**6. Коннекторы к файловым системам**](#part_06)\n",
    "    - [6.1 Введение](#part_61)\n",
    "    - [6.2 Возможности коннекторов](#part_62)\n",
    "    - [6.3 Обзор коннекторов](#part_63)\n",
    "    - [6.4 Демо](#part_64)\n",
    "\n",
    "- [**7. Объект скачивания данных FileDownloader**](#part_07)\n",
    "    - [7.1 Объекты манипуляции данными](#part_71)\n",
    "    - [7.2 FileDownloader](#part_72)\n",
    "    - [7.3 Конструктор FileDownloader()](#part_73)\n",
    "    - [7.4 FileDownloader Options](#part_74)\n",
    "    - [7.5 Методы FileDownloader](#part_75)\n",
    "    - [7.6 FileDownloader Result](#part_76)\n",
    "    - [7.7 Демо](#part_77)\n",
    "\n",
    "- [**8. Объект загрузки данных FileUploader**](#part_08)\n",
    "    - [8.1 Объекты манипуляции данными](#part_81)\n",
    "    - [8.2 FileUploader](#part_82)\n",
    "    - [8.3 Конструктор FileUploader()](#part_83)\n",
    "    - [8.4 FileUploader Options](#part_84)\n",
    "    - [8.5 Методы FileUploader](#part_85)\n",
    "    - [8.6 FileUploader Result](#part_86)\n",
    "    - [8.7 Демо](#part_87)\n",
    "\n",
    "- [**9. FileMover, File Filters & File Limits**](#part_09)\n",
    "    - [9.1 Объекты манипуляции данными](#part_91)\n",
    "    - [9.2 FileMover](#part_92)\n",
    "    - [9.3 Конструктор FileMover()](#part_93)\n",
    "    - [9.4 FileMover Options](#part_94)\n",
    "    - [9.5 Методы FileMover](#part_95)\n",
    "    - [9.6 FileMover Result](#part_96)\n",
    "    - [9.7 FileFilters](#part_97)\n",
    "    - [9.8 FileLimits](#part_98)\n",
    "    - [9.9 Демо](#part_99)\n",
    "    \n",
    "- [**10. HWM & HWM Store. Snapshot strategy & Incremental strategy. Batch strategies**](#part_010)\n",
    "    - [10.1 HWM - High Water Mark](#part_101)\n",
    "    - [10.2 HWMStore](#part_102)\n",
    "    - [10.3 YAMLHWMStore](#part_103)\n",
    "    - [10.4 Стратегии чтения ](#part_104)\n",
    "    - [10.5 Стратегия Snapshot](#part_105)\n",
    "    - [10.6 Стратегия Snapshot Batch](#part_106)\n",
    "    - [10.7 Стратегия Incremental](#part_107)\n",
    "    - [10.8 Стратегия IncrementalBatch](#part_108)\n",
    "    - [10.9 Демо](#part_109)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa25791",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4f8d7",
   "metadata": {},
   "source": [
    "<a id='part_01'></a>\n",
    "# 1. Введение в onETL [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d2b2e",
   "metadata": {},
   "source": [
    "<a id='part_11'></a>\n",
    "## 1.1 Описание базовых функций [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1fddb0",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "В основе библиотеки `onETL` лежит движок `Apache Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b9bbd",
   "metadata": {},
   "source": [
    "![](data/onetl_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56959e7c",
   "metadata": {},
   "source": [
    "### Группы функциональностей\n",
    "\n",
    "- `DBConnections` - подключения к БД и похожим на них хранилищам\n",
    "- `FileConnections` - подключение к раздичным файловым системам\n",
    "- `FileDataFrameConnections` - подключение к раздичным файловым системам\n",
    "- `DBClasses` - объекты чтения и записи в БД\n",
    "- `FileClasses` - объекты манипуляции данными на файловых системах\n",
    "- `FileDataFrameClasses` - объекты чтения DF и записи их\n",
    "- `HWM`, `Read Strategies`, `HWM Store` - объекты поддержки инкриментальных процессов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140dc54f",
   "metadata": {},
   "source": [
    "### DB Connections\n",
    "\n",
    "Подключения к БД и похожим на них хранилищам\n",
    "\n",
    "- **Реляционные**\n",
    "    - `Postgres`\n",
    "    - `GreenPlum`\n",
    "    - `Oracle`\n",
    "    - `MSSQL`\n",
    "    - `MySQL`\n",
    "- **Не реляционные хранилища**\n",
    "    - `Clickhouse`\n",
    "    - `Teradata`\n",
    "    - `Give`\n",
    "    - `Kafka`\n",
    "    - `MongoDB`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4f1e4",
   "metadata": {},
   "source": [
    "### File Connections\n",
    "\n",
    "Подключение к раздичным файловым системам\n",
    "- `FTP`\n",
    "- `FTPS`\n",
    "- `SFTP`\n",
    "- `HDFS`\n",
    "- `S3`\n",
    "- `WedDAV`\n",
    "- `Samba`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17391e6f",
   "metadata": {},
   "source": [
    "### File DataFrame Connections\n",
    "\n",
    "Данный коннектор позволяет читать данные из файлов напрямую в DataFrame\n",
    "\n",
    "Поддерживаются на источниках:\n",
    "- `Локальная файловая система`\n",
    "- `HDFS`\n",
    "- `S3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9c90c",
   "metadata": {},
   "source": [
    "### DBClasses\n",
    "\n",
    "Объекты чтения и записи в БД\n",
    "- `DBReader` - для чтения данных\n",
    "- `DBWriter` - для записи данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de50a7e",
   "metadata": {},
   "source": [
    "### FileClasses\n",
    "\n",
    "Для манипуляции с файлами, но не для их чтения или записи\n",
    "\n",
    "- `File Downloader` - для скачивания файлов\n",
    "- `File Uploader` - для загрузки файлов\n",
    "- `File Mover` - для перемещения по файловой системе\n",
    "- `Filters`- фильтры списка файлов\n",
    "- `Limits` - лимит количества файлов, которое планируется обработать\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a28ce",
   "metadata": {},
   "source": [
    "### FileDataFrameClasses\n",
    "\n",
    "Чтение данных из объектов DataFrame\n",
    "\n",
    "- `FileDF Reader` \n",
    "- `FileDF Writer` \n",
    " \n",
    "Виды файлов с которыми умеет работать библиотека можно разбить на 3 группы\n",
    "- Часто применяемые в BigData\n",
    "    - `Avro`\n",
    "    - `ORC`\n",
    "    - `Parquet`\n",
    "- Широко распространненые форматы обмена данными\n",
    "    - `XML`\n",
    "    - `JSON`\n",
    "    - `JSONLine`\n",
    "    - `CSV`\n",
    "- Преимущественно пользовательский\n",
    "    - `Excel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2de5f",
   "metadata": {},
   "source": [
    "### HWM\n",
    "\n",
    "`High Water Mark` - поддержка инкрементных процессов\n",
    "\n",
    "Поддерживаемые типы `HWM` меток:\n",
    "- `Integer`\n",
    "- `Decimal`\n",
    "- `Date`\n",
    "- `Datetime`\n",
    "\n",
    "Поддерживаемые стратегии чтения `Read Strategies`:\n",
    "- `Snapshot strategy`\n",
    "- `Incremetal strategy`\n",
    "- `Snapshot batch stategy`\n",
    "- `Incremental batch strategy`\n",
    "\n",
    "Возможность работы с несколькими видами хранилищ меток `HWM Store`\n",
    "- `YAML HWM Store`\n",
    "- `In-memory HWM Store`\n",
    "- `DataOps.ETL Service`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b23f",
   "metadata": {},
   "source": [
    "**Ссылка на библиотеку:** https://onetl.readthedocs.io/en/0.13.3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c8b42",
   "metadata": {},
   "source": [
    "<a id='part_12'></a>\n",
    "## 1.2 Настройка [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e401a",
   "metadata": {},
   "source": [
    "### Компетенции\n",
    "\n",
    "Компетенции необходимые для работы с библиотекой `onETL`\n",
    "- `python`\n",
    "- `pip`\n",
    "- `Apache Spark` https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307e5d2",
   "metadata": {},
   "source": [
    "### Установка библиотеки\n",
    "\n",
    "- Первое, необходимо убедиться что в окружении присутствует `JAVA` совсместимая со `Spark` версией.\n",
    "- Список поддерживаемых версий JAVA можно узнать по ссылке: [**`https://spark.apache.org/docs/latest/`**](https://spark.apache.org/docs/latest/)\n",
    "- На текущий момент это версии 8, 11 и 17\n",
    "\n",
    "Убедимся что у нас подходящая версия JAVA\n",
    "\n",
    "для этого в консоли выполним команду\n",
    "\n",
    "```bash\n",
    "java -version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d6138",
   "metadata": {},
   "source": [
    "![](data/onetl_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549fcd2",
   "metadata": {},
   "source": [
    "### Установка в контуре МТС\n",
    "\n",
    "Поскольку onETL опубликована в открытом доступе, нам понадобится возможность использовать пакеты из внешних источников, в рамках нашей корпоративной сети нужно настроить `pip` на работу с `nexus` (используемую в компании платформу управления репозиториями)\n",
    "\n",
    "Чтобы это сделать, нужно чтобы в конфигурационном файле `pip.conf` были указаны адреса сервисов `nexus` и `artifactory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4f895",
   "metadata": {},
   "source": [
    "![](data/onetl_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd706b98",
   "metadata": {},
   "source": [
    "Обычно `pip.conf` создают в корневой папке вирутального окружения, однако есть и другие варианты расположения `pip.conf` узнать о них можно по ссылке: \n",
    "\n",
    "[**`https://pip.pypa.io/en/stable/topics/configuration/#location`**](https://pip.pypa.io/en/stable/topics/configuration/#location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2c66c",
   "metadata": {},
   "source": [
    "### Бандлы onETL\n",
    "\n",
    "```bash\n",
    "pip install onetl\n",
    "pip install onetl[spark]\n",
    "pip install onetl[kerberos]\n",
    "pip install onetl[files]\n",
    "pip install onetl[all]\n",
    "```\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/install/index.html`**](https://onetl.readthedocs.io/en/stable/install/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092f346",
   "metadata": {},
   "source": [
    "### Поддержка Kerberos\n",
    "\n",
    "Если необходима поддержка `Kerberos` необъодимо установить дополнительные пакеты \n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/install/kerberos.html`**](https://onetl.readthedocs.io/en/stable/install/kerberos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8a805",
   "metadata": {},
   "source": [
    "### MTSpark\n",
    "\n",
    "Если планируется использовать `onETL` с данными из корпоративных кластеров `BigData` то стоит установить библиотеку `MTSpark` облегчающую подключение к ним\n",
    "\n",
    "[**`https://confluence.mts.ru/pages/viewpage.action?pageId=723527060`**](https://confluence.mts.ru/pages/viewpage.action?pageId=723527060)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8836ff2",
   "metadata": {},
   "source": [
    "### Совместимость со Spark\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/install/spark.html#compatibility-matrix`**](https://onetl.readthedocs.io/en/stable/install/spark.html#compatibility-matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908dc64",
   "metadata": {},
   "source": [
    "<a id='part_13'></a>\n",
    "## 1.3 Установка [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f0446",
   "metadata": {},
   "source": [
    "Для того чтобы установить библиотеку в командной строке пишем:\n",
    "\n",
    "```bash\n",
    "pip install onetl[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c5dfe",
   "metadata": {},
   "source": [
    "![](data/onetl_004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587532fa",
   "metadata": {},
   "source": [
    "<a id='part_14'></a>\n",
    "## 1.4 Архитектура Spark [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b4a1c",
   "metadata": {},
   "source": [
    "Архитектура Spark предполагает два базовых варианта его использования\n",
    "\n",
    "- **Запуск в режиме кластера:**\n",
    "    - `под упралвнием Yarn`\n",
    "    - `под управлением k8s`\n",
    "    - `Spark Standalone` - управляет сам Spark\n",
    "    - `Mesos (deprecated)` - устаревший\n",
    "- **Безкластерный Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485ccc8",
   "metadata": {},
   "source": [
    "![](data/onetl_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee969c",
   "metadata": {},
   "source": [
    "[**`https://spark.apache.org/docs/latest/cluster-overview.html`**](https://spark.apache.org/docs/latest/cluster-overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b6bf3",
   "metadata": {},
   "source": [
    "<a id='part_15'></a>\n",
    "## 1.5 Запуск сессии Spark [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df851d",
   "metadata": {},
   "source": [
    "### Локальная Spark-сессия\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "                     .appName(\"check-local-spark\")\n",
    "                     .master(\"local[1]\")\n",
    "                     .getOrCreate())\n",
    "print(spark)\n",
    "print(spark.catalog.listTables())\n",
    "print(spark.sql(\"select 'Hello spark' AS test_column\").collect())\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2983a",
   "metadata": {},
   "source": [
    "### Сессия Spark с использованием кластера\n",
    "\n",
    "\n",
    "```python\n",
    "from mtspark import kinit     # функция kinit позволяет нам авторизоваться на кластере\n",
    "from mtspark import get_spark # поднимает сессию\n",
    "\n",
    "kinit() # после вызова этой функции приложение запросит пароль для авторизации\n",
    "\n",
    "spark = get_spark({\"appName\": \"check-cluster-spark\"}, \n",
    "                  spark_version=\"3.2.0\")\n",
    "spark.catalog.listTables()\n",
    "spark.stop()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd420b16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135259a",
   "metadata": {},
   "source": [
    "<a id='part_02'></a>\n",
    "# 2. Коннекторы к БД [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4262a77",
   "metadata": {},
   "source": [
    "<a id='part_21'></a>\n",
    "## 2.1 Введение [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1e122",
   "metadata": {},
   "source": [
    "- Какие бывают типы коннекторов и как их импортировать\n",
    "- Где выполняются методы объекта коннектора\n",
    "- Зачем нужны коннекторы \n",
    "- В чем их отличия от объектов чтения или записи данных\n",
    "- Как подключать пакеты драйверов Spark и как они зависят от его версии\n",
    "\n",
    "Коннекторы - это объекты подключения\n",
    "- `DBConnections` - подключения к БД и похожим на них хранилищам\n",
    "- `FileConnections` - подключение к раздичным файловым системам\n",
    "- `FileDataFrameConnections` - подключение к раздичным файловым системам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddf6aa",
   "metadata": {},
   "source": [
    "<a id='part_22'></a>\n",
    "## 2.2 Логирование [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e8646",
   "metadata": {},
   "source": [
    "Для более информативного взаимодействия с библиотекой для начала настроим логирование \n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/logging.html`**](https://onetl.readthedocs.io/en/stable/logging.html)\n",
    "\n",
    "В библиотеке реализован функционал для работы с логированием, лог формируется в удобном для чтения и анализа виде и содержит исчерпывающую для изучения информацию.\n",
    "\n",
    "### Функции логирования\n",
    "\n",
    "```python\n",
    "setup_logging()\n",
    "setup_clients_logging()\n",
    "set_default_logging_format()\n",
    "```\n",
    "\n",
    "### `setup_logging()`\n",
    "\n",
    "Функция предназна для использования как в среде разработки так и для использования в регулярных процессах.\n",
    "\n",
    "- функция добавляет обработчик стандартного вывода исключений\n",
    "- меняет формат вывода лога к указанному виду\n",
    "\n",
    "**Параметры**\n",
    "- уровень логирования\n",
    "- активация логирования на клиентах\n",
    "\n",
    "**Устанавливает**\n",
    "- обработчик stdeer\n",
    "- формат `\"2023-05-31 11:22:33.456 [INFO] MainThread: message\"`\n",
    "- корневой уровень логирования\n",
    "- уровень логирвоания onETL\n",
    "- уровень логирования клиентов\n",
    "\n",
    "### `setup_clients_logging()`\n",
    "\n",
    "Функция устанавливает уровень логирования в используемых onETL клинетских модулях\n",
    "\n",
    "Модули перечислены ниже:\n",
    "\n",
    "**Параметры**\n",
    "- уровень логирования\n",
    "\n",
    "**Влияет на**\n",
    "- `ftputil`\n",
    "- `hdfs`\n",
    "- `minio`\n",
    "- `paramiko`\n",
    "- `py4j`\n",
    "- `pyspark`\n",
    "- `weddav3`\n",
    "\n",
    "\n",
    "### `set_default_logging_format()`\n",
    "\n",
    "- Устанавливает формат логирования вида `\"2023-05-31 11:22:33.456 [INFO] MainThread: message\"`\n",
    "\n",
    "Предназначено для использования в среде разработки (IDE) или процессах ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ac756",
   "metadata": {},
   "source": [
    "### Настройка логирования\n",
    "\n",
    "**для 90% случаев этого будет достаточно**\n",
    "\n",
    "```python\n",
    "from onetl.log import setup_logging\n",
    "setup_logging()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Чтобы писать лог в файл, можно использовать следующую конструкцию:**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from onetl.log import setup_logging\n",
    "\n",
    "# Вызов функции настройки логирования\n",
    "setup_logging()\n",
    "\n",
    "# Дополнительная конфигурация для записи логов в файл\n",
    "logger = logging.getLogger('onetl')  # Замените 'onetl' на фактическое имя логгера, используемого библиотекой\n",
    "file_handler = logging.FileHandler('my_onetl_log.log')  # Укажите имя файла для логов\n",
    "\n",
    "# Опционально: Установите уровень сообщений, которые следует записывать\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Форматирование сообщений лога\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Добавьте обработчик файла к логгеру\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "```\n",
    "\n",
    "**Остановаить логирование**\n",
    "\n",
    "```python\n",
    "# Продолжаем использовать тот же логгер, который был создан ранее\n",
    "logger = logging.getLogger('onetl')\n",
    "\n",
    "# Просмотр всех текущих обработчиков\n",
    "for handler in logger.handlers:\n",
    "    if isinstance(handler, logging.FileHandler):\n",
    "        # Удаление обработчика файла, чтобы остановить запись в файл\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()  # Важно: Закрыть файл, чтобы освободить ресурсы\n",
    "\n",
    "# Теперь логи больше не будут записываться в файл my_onetl_log.log\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a0280",
   "metadata": {},
   "source": [
    "<a id='part_23'></a>\n",
    "## 2.3 DB Connections [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af2180",
   "metadata": {},
   "source": [
    "Все коннекторы живут в модуле **`onetl.connection`** из этого модуля можно импортировать любой коннектор с которым планируется работать вне зависимости от типа хранилища\n",
    "\n",
    "```python\n",
    "from onetl.connection import ...\n",
    "```\n",
    "\n",
    "Сначала рассмотрим обычные коннекторы, такие как\n",
    "\n",
    "- `MSSQL`\n",
    "- `Postgres`\n",
    "- `MySQL`\n",
    "- `Oracle`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f1c46",
   "metadata": {},
   "source": [
    "### Дока\n",
    "\n",
    "- [**`MSSQL`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mssql/connection.html)\n",
    "- [**`postgreSQL`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/postgres/connection.html)\n",
    "- [**`MySQL`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mysql/connection.html)\n",
    "- [**`Oracle`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/oracle/connection.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485005a7",
   "metadata": {},
   "source": [
    "### Конструктор\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabfcc9",
   "metadata": {},
   "source": [
    "Конструктор объекта коннектора не открывает соединение, а только описывает его. Коннектор реализован как контекстный менеджер, его можно использовать с конструкцией ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c24f72",
   "metadata": {},
   "source": [
    "![](data/onetl_006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e04de8",
   "metadata": {},
   "source": [
    "### Параметр `extra`\n",
    "\n",
    "`extra` это специфические для каждого коннектора ньюансы и описываются они в документации соответствующих драйверов JDBC\n",
    "\n",
    "Ниже ссылки на страницы JDBC драйверов\n",
    "- [**`extra MSSQL`**](https://learn.microsoft.com/en-us/sql/connect/jdbc/setting-the-connection-properties?view=sql-server-ver16#properties)\n",
    "- [**`extra PostgreSQL`**](https://github.com/pgjdbc/pgjdbc#connection-properties)\n",
    "- [**`extra MySQL`**](https://dev.mysql.com/doc/connector-j/en/connector-j-reference-configuration-properties.html)\n",
    "- [**`extra Orcle`**](https://docs.oracle.com/en/database/oracle/oracle-database/23/jajdb/oracle/jdbc/OracleConnection.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b85356",
   "metadata": {},
   "source": [
    "<a id='part_24'></a>\n",
    "## 2.4 Методы коннекторов [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eca6d1",
   "metadata": {},
   "source": [
    "Коннектор реализует общие методы\n",
    "\n",
    "- `check()`\n",
    "- `get_packages()`\n",
    "- `sql()`\n",
    "- `fetch()`\n",
    "- `execute()`\n",
    "- `close()`\n",
    "\n",
    "\n",
    "### `check()`\n",
    "\n",
    "Позволяет убедиться что соединение установлено\n",
    "\n",
    "**`MSSQL`**\n",
    "```sql\n",
    "SELECT 1 AS field\n",
    "```\n",
    "\n",
    "\n",
    "**`Oracle`**\n",
    "```sql\n",
    "SELECT 1 FROM dual\n",
    "```\n",
    "\n",
    "\n",
    "**`PostgreSQL` и `MySQL`**\n",
    "```sql\n",
    "SELECT 1\n",
    "```\n",
    "\n",
    "**Результат `check()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf14b4",
   "metadata": {},
   "source": [
    "![](data/onetl_007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6c538",
   "metadata": {},
   "source": [
    "### `get_packages()`\n",
    "\n",
    "Чтобы подключаться к базам данных `Spark` использует `JBDC` драйверы, а еще он умеет получать из `Maven` зависимости, которые требужются для работы приложения. \n",
    "\n",
    "Для нас это означает что при создании сессии `Spark` мы можем указать `JDBC` драйвер требующийся нам для подключения к базе данных и чтобы упростить этот момент в библиотеке реализован метод `get_packages()`\n",
    "\n",
    "Этот метод как раз и возвращает имя пакета `JDBC` драйвера из числа протестированных в рамках разработки библиотеки `onETL`.\n",
    "\n",
    "Обратите внимание что в случае `Oracle` и `MSSQL` можно указать версию JAVA, которую мы используем, драйвера этих СУБД отличаются для разных версий JAVA, по умолучанию предполагается что версия JAVA восьмая."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba761e2",
   "metadata": {},
   "source": [
    "![](data/onetl_008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327043e2",
   "metadata": {},
   "source": [
    "### `ivysettings.xml` - ВАЖНО!!!\n",
    "\n",
    "Небольшая оговорка про использование в корпоративной среде MTS\n",
    "\n",
    "Чтобы `Spark` мог получать пакеты из `Maven` так же как и в случае с **`pip`** ему нужно подсказать какие корпоративные сервисы для этого использовать. \n",
    "\n",
    "Чтобы это сделать создайте файл **`ivysettings.xml`** со следующим содержимым:\n",
    "\n",
    "```xml\n",
    "<ivysettings>\n",
    "    <settings defaultResolver=\"main\"/>\n",
    "    <resolvers>\n",
    "        <chain name=\"main\" returnFirst=\"true\">\n",
    "            <!-- Use Maven cache -->\n",
    "            <ibiblio name=\"local-maven-cache\" m2compatible=\"true\" root=\"file://${user.home}/.m2/repository\"/>\n",
    "            <!-- Use ~/.ivy2/jars/*.jar files -->\n",
    "            <ibiblio name=\"local-ivy2-cache\" m2compatible=\"false\" root=\"file://${user.home}/.ivy2/jars\"/>\n",
    "            <!-- BigData packages -->\n",
    "            <ibiblio name=\"bigdata\" m2compatible=\"true\" root=\"https://artifactory.mts.ru/artifactory/maven-bigdata/\" />\n",
    "            <!-- Nexus package proxy -->\n",
    "            <ibiblio name=\"nexus-proxy-maven\" m2compatible=\"true\" root=\"https://nexus.services.mts.ru/repository/maven-central/\" />\n",
    "            <ibiblio name=\"nexus-proxy-apache\" m2compatible=\"true\" root=\"https://nexus.services.mts.ru/repository/maven-apache/\" />\n",
    "            <ibiblio name=\"nexus-proxy-jcenter\" m2compatible=\"true\" root=\"https://nexus.services.mts.ru/repository/maven-jcenter/\" />\n",
    "        </chain>\n",
    "    </resolvers>\n",
    "</ivysettings>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0313aa",
   "metadata": {},
   "source": [
    "![](data/onetl_009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38244817",
   "metadata": {},
   "source": [
    "### Spark-сессия с `ivysettings` - ВАЖНО!!!!\n",
    "\n",
    "После этого при старте Spark сессии обязательно надо указать путь до файла **`ivysettings.xml`**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession  # импортируем установленный Spark\n",
    "from onetl.connection import Postgres\n",
    "from onetl.log import setup_logging\n",
    " \n",
    "# настраиваем формат логов и уровень логирования\n",
    "setup_logging()\n",
    " \n",
    "# создаем сессию\n",
    "spark = (\n",
    "  SparkSession.builder\n",
    "  .config(\"spark.master\", \"local[*]\")  # локальный запуск, используются все доступные ядра (между [] можно указать их количество)\n",
    "  .config(\"spark.appName\", \"mysessionname\")  # название сессии\n",
    "  .config(\"spark.jars.packages\", \",\".join(Postgres.get_packages()))  # перечисляем пакеты для подключения к конкретному типу БД\n",
    "  .config(\"spark.jars.ivySettings\", \"/home/myuser/ivysettings.xml\")  # путь к ivysettings.xml\n",
    "  .getOrCreate()\n",
    ")\n",
    " \n",
    "# проверяем, жива ли сессия\n",
    "print(spark.sql(\"select 'Hello spark'\").collect()[0][0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd3456",
   "metadata": {},
   "source": [
    "[**Ссылка на доку**](https://confluence.mts.ru/pages/viewpage.action?pageId=723527140#id-ДокументацияonETL-Влюбомдругомокружении(накорпоративномноутбуке,вVDI,навиртуалкезаBalabit,вdockerобразе))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff9998",
   "metadata": {},
   "source": [
    "![](data/onetl_010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41a756",
   "metadata": {},
   "source": [
    "### `close()`\n",
    "\n",
    "Закрывает соединения открытые методами \n",
    "- `check()`\n",
    "- `fetch()`\n",
    "- `execute()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2134d",
   "metadata": {},
   "source": [
    "<a id='part_25'></a>\n",
    "## 2.5 Read and Write Options [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d2d87",
   "metadata": {},
   "source": [
    "Операции чтения и записи поддерживают опции, можно передать любые которые поддерживает Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf43d9",
   "metadata": {},
   "source": [
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html`**](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f24816",
   "metadata": {},
   "source": [
    "### Опции чтения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed87994",
   "metadata": {},
   "source": [
    "![](data/onetl_011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84d9ae",
   "metadata": {},
   "source": [
    "### Опции записи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473efee",
   "metadata": {},
   "source": [
    "![](data/onetl_012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d1907",
   "metadata": {},
   "source": [
    "### `sql()`\n",
    "\n",
    "Метод `sql()` предназначен для выполнения запросов к источнику данных. Принимает в качестве параметра текст запроса. Этот метод не поддерживает стратегии чтения он выполняется на `executor` и как большинство операций в Spark является ленивым.\n",
    "\n",
    "- Выполняется на `executor`\n",
    "- Параметры\n",
    "    - `query`\n",
    "        - только `SELECT`\n",
    "        - применяется синтаксис конкретной СУБД\n",
    "        - `SHOW` не поддерживается\n",
    "    - `options`\n",
    "        - `ReadOptions`\n",
    "- Возвращает `DataFrame`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37eb01",
   "metadata": {},
   "source": [
    "### `fetch()`\n",
    "\n",
    "Метод `fetch()` предназначен для получения небольших фрагментов данных. Параметры и результаты его выполнения похожи на метод `sql()`. \n",
    "\n",
    "Однако использовать этот метод надо аккуратно, так как этот метод выполняется на драйвере. Не используйте этот метод для получения большого объема данных, так как Spark попытается их разместить в оперативной памяти хоста на которой работает драйвер. Эта операция в отличии от предыдущей выполняется немедленно (не является ленивой)\n",
    "\n",
    "- Выполняется на `driver`\n",
    "- Параметры\n",
    "    - `query`\n",
    "        - `SELECT + SHOW`\n",
    "        - применяется синтаксис конкретной СУБД\n",
    "    - `options`\n",
    "        - `ReadOptions`\n",
    "- Возвращает `DataFrame`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2364e",
   "metadata": {},
   "source": [
    "### `execute()`\n",
    "\n",
    "Метод `execute()` - предназначен для выполнения процедур или инструкций определения структуры данных (DDL). Метод выполняется на драйвере, по этому использовать этот метод необходимо с аккуратностью. Данные возвращаются только в тех случаях когда это явно предусмотрено инструкцией передаваемой в `query` или в случае выполнении процедуры.\n",
    "\n",
    "- Выполняется на `driver`\n",
    "- Параметры\n",
    "    - `query`\n",
    "        - поддерживается все, кроме `SELECT`\n",
    "        - применяется синтаксис конкретной СУБД\n",
    "    - `options`\n",
    "        - `ReadOptions`\n",
    "- Возвращает `DataFrame` (только в случает инструкции `RETURNING` или вызова процедуры)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb0bf2",
   "metadata": {},
   "source": [
    "### Отличия коннекторов от объектов манипуляции данными\n",
    "\n",
    "- Основная задача коннекторов устанавливать соединение\n",
    "- Основная задача объектов DBReader/DBWriter считывать или записывать данные соответственно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dec699",
   "metadata": {},
   "source": [
    "![](data/onetl_014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e11c3a",
   "metadata": {},
   "source": [
    "<a id='part_26'></a>\n",
    "## 2.6 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979e237",
   "metadata": {},
   "source": [
    "![](data/onetl_013.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340834c6",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Postgres\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# from omegaconf import OmegaConf \n",
    "# данная библиотека позволяет получать параметры подключения из файла \n",
    "# тут приведена для примера\n",
    "\n",
    "from onetl.log import setup_logging # настраиваем логирование\n",
    "\n",
    "\n",
    "# Spark использует JBDC драйверы для подключения.\n",
    "# Нам необходимо знать с помощью какого драйвера подключение может быть осуществлено\n",
    "# для этого используем функцию .get_packages() которая возвращает такое имя\n",
    "maven_packages = Postgres.get_packages()\n",
    "\n",
    "\n",
    "# Создаем Spark сессию\n",
    "spark = (SparkSession.builder\n",
    "                     .appName(\"PostgresExample\")\n",
    "                     .master(\"local[1]\") # запуск локальной сессии\n",
    "                     .config(\"spark.jars.packages\", \",\".join(maven_packages)) # распаковка пакетов\n",
    "                     .config(\"spark.jars.ivySettings\", \"ivysettings.xml\")     # путь к ivysettings.xml обязательно внутри контура МТС\n",
    "                     .getOrCreate())\n",
    "\n",
    "# пример создания подключения с помощью OmegaConf\n",
    "# conf = OmegaConf.load(\"demo_param.yaml\")\n",
    "# postgres = Postgres(**conf, spark=spark)\n",
    "\n",
    "\n",
    "# Создаем объект коннектора\n",
    "# создание объекта коннектора не создает самого соединения\n",
    "postgres = Postgres(\n",
    "    host     = \"gpm-001.camr.mts-corp.ru\",\n",
    "    port     = 5432,\n",
    "    user     = username,\n",
    "    password = passw,\n",
    "    database = \"gto\",\n",
    "    spark    = spark\n",
    ")\n",
    "\n",
    "\n",
    "postgres.check() # Чтобы соединение установилось выполним метод .check()\n",
    "\n",
    "# пробуем получить данные из Postgres используя метод .sql() который выполняется на executor\n",
    "df = postgres.sql('SELECT Дата, День_недели, День_месяца, Месяц_число FROM gto.dic.calendar LIMIT 1')\n",
    "\n",
    "# не смотря на то что ячейка выполнилась, данные мы пока не получили\n",
    "# метод .sql() является ленивым и без явного обращения к данным выполнение висит на паузе\n",
    "\n",
    "\n",
    "df.show(truncate=False) # Фактическое выполнение запроса\n",
    "\n",
    "# Попробуем другой метод, например .fetch() \n",
    "# его отличие в том, что он выполняется на драйвере и он выполняется немедленно\n",
    "# параметр \"show all\" вернет список всех параметров конфигурации PostgreSQL вместе с их текущими значениями\n",
    "df = postgres.fetch(\"show all\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "postgres.close()\n",
    "\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d0788",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301425f",
   "metadata": {},
   "source": [
    "<a id='part_03'></a>\n",
    "# 3. Объекты манипуляции данными DBReader и DBWriter [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e845f",
   "metadata": {},
   "source": [
    "<a id='part_31'></a>\n",
    "## 3.1 DBReader() [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53905c",
   "metadata": {},
   "source": [
    "Документация - [**`https://onetl.readthedocs.io/en/stable/db/db_reader.html`**](https://onetl.readthedocs.io/en/stable/db/db_reader.html)\n",
    "\n",
    "- Объект `DBReader` читает данные из источника в `Spark DataFrame`\n",
    "- Имеет метод **`run()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d1144",
   "metadata": {},
   "source": [
    "### Нюансы\n",
    "\n",
    "- В зависимости от стратегии чтения `DBReader` может возвращать различающийся результат\n",
    "- DBReader читает данные только из одной таблицы за 1 раз\n",
    "- Плюсами метода являются метки `HWM` и схема набора данных `df_schema`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2cc45",
   "metadata": {},
   "source": [
    "### Конструктор\n",
    "\n",
    "### `DBReader()` - параметры\n",
    "\n",
    "- `connection` - это объект соединения - коннектор\n",
    "- `sourse(alias\"table\")` - bсточник данных\n",
    "\n",
    "### `DBReader()` - `cloumns`\n",
    "\n",
    "Список столбцов для извлечения. Если система хранения поддерживает выражения то их тоже можно использовать.\n",
    "\n",
    "```python\n",
    "columns = [\n",
    "    \"id_name\",\n",
    "    \"another_column as alias\",\n",
    "    \"count(*) over()\",\n",
    "    \"some(function) as alias2\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a593313",
   "metadata": {},
   "source": [
    "### `DBReader()` - `where`\n",
    "\n",
    "Where  - это условия применяемое к извлекаемым данным. Синтаксис в этом параметре зависит от используемой системы хранения.\n",
    "\n",
    "**MySQL**\n",
    "\n",
    "```python\n",
    "where = \"column_1 > 2\"\n",
    "]\n",
    "```\n",
    "\n",
    "**MongoDB**\n",
    "\n",
    "```python\n",
    "where = {\n",
    "    \"col_1\": {\"$gt\": 1, \"$lt\": 100},\n",
    "    \"col_2\": {\"$gt\": 2},\n",
    "    \"col_3\": {\"$eq\": \"hello\"},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea337a",
   "metadata": {},
   "source": [
    "### `DBReader()` - `hwm`\n",
    "\n",
    "В этом уроке мы не будем подробно останавливаться на инкрементальных процессах. Об этом будет отдельный урок.\n",
    "\n",
    "```python\n",
    "from onetl.hwm import AutoDetectHWM\n",
    "\n",
    "hwm = AutoDetectHWM(\n",
    "    name=\"some_unicue_hwm_name\",\n",
    "    expression=\"hwm_column\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f43561",
   "metadata": {},
   "source": [
    "### `DBReader()` - `hint`\n",
    "\n",
    "Хинты позволяют нам оптимизировать работу с данными на лету. Как правило spark хорошо справляется со своими задачами, но все же есть возможность задейстовать средства, которые предоставляют нам системы хранения. Их синтаксис зависит от используемой системы хранения.\n",
    "\n",
    "**Oracle**\n",
    "\n",
    "```python\n",
    "hint = \"index(myschema.mytable mycolumn)\"\n",
    "```\n",
    "\n",
    "**MongoDB**\n",
    "\n",
    "```python\n",
    "hint = {\n",
    "    \"mycolumn\": 1,\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4e81f",
   "metadata": {},
   "source": [
    "### `DBReader()` - `df_schema`\n",
    "\n",
    "В DBReader у нас есть возможность определить типы данных для колонок получаемого датафрейма. Мы можем это сделать передав в конструктор параметр `df_schema`. Использование этого параметра поддерживается не каждой системой хранения.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types.import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"_id\", IntegerType()),\n",
    "        StructField(\"text_string\", StringType()),\n",
    "        StructField(\"hwm_int\", IntegerType()),\n",
    "        StructField(\"hwm_datetime\", TimestampType()),\n",
    "        StructField(\"float_value\", DoubleType()),\n",
    "    ],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d434d02",
   "metadata": {},
   "source": [
    "### `DBReader()` - `options`\n",
    "\n",
    "Параметр `options` принимает словарь с поддерживаемыми spark опциями чтения. Праметры чтения и записи поддерживают опции, можно передать любые которые поддерживает spark. \n",
    "\n",
    "Документация - [**`https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html`**](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "\n",
    "Для ряда опций в библиотеке установлены значения по умолчанию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad778a",
   "metadata": {},
   "source": [
    "![](data/onetl_022.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd3aa2",
   "metadata": {},
   "source": [
    "### `DBReader().run()`\n",
    "\n",
    "Метод `.run()` он очень простой, не принимает никаких параметров и возвращает датафрейм.\n",
    "\n",
    "```python\n",
    "from onetl.db import DBReader\n",
    "from onetl.connection import Hive\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"spark-app-name\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(connection=hive, sourse=\"fiddle.dummy\")\n",
    "df = reader.run()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1697c6",
   "metadata": {},
   "source": [
    "<a id='part_32'></a>\n",
    "## 3.2 DBWriter() [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0e960",
   "metadata": {},
   "source": [
    "Объект `DBWriter` мы используем для того чтобы записать имеющийся датафрейм в систему хранения\n",
    "\n",
    "Документация - [**`https://onetl.readthedocs.io/en/stable/db/db_writer.html`**](https://onetl.readthedocs.io/en/stable/db/db_writer.html)\n",
    "\n",
    "Предоставляет:\n",
    "- конструктор\n",
    "- метод `run(df)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601db0c",
   "metadata": {},
   "source": [
    "### Конструктор DBWriter()\n",
    "\n",
    "### Параметры\n",
    "\n",
    "- `connection` коннектор\n",
    "- `target(alias\"table\")` - Таблица приемник\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103655f",
   "metadata": {},
   "source": [
    "### `DBWriter()` - `options`\n",
    "\n",
    "Параметр options принимает словарь с поддерживаемыми spark опциями записи. Праметры чтения и записи поддерживают опции, можно передать любые которые поддерживает spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9463d4",
   "metadata": {},
   "source": [
    "![](data/onetl_023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6dd44",
   "metadata": {},
   "source": [
    "### `DBWriter().run(df)`\n",
    "\n",
    "Принимает единственный параметр - датафрейм\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.db import DBReader, DBWriter\n",
    "from onetl.connection import Hive\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .appName(\"spark-app-name\")\n",
    "                     .enableHiveSupport()\n",
    "                     .getOrCreate()\n",
    ")\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(connection=hive, sourse=\"fiddle.dummy\")\n",
    "df = reader.run()\n",
    "\n",
    "options = {\"compression\":\"snappy\", \"partitionBy\":\"id\"}\n",
    "\n",
    "writer = DBWriter(\n",
    "    connection=hive,\n",
    "    target=\"default.test\",\n",
    "    options=options\n",
    ")\n",
    "\n",
    "writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5496b6",
   "metadata": {},
   "source": [
    "<a id='part_33'></a>\n",
    "## 3.3 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf51fc",
   "metadata": {},
   "source": [
    "```python\n",
    "import yaml # для передачи зашифрованных учетных даных\n",
    "from omegaconf import DictConfig # для передачи зашифрованных учетных даных\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from onetl.connection import Postgres, Clickhouse\n",
    "from onetl.db import DBWriter, DBReader\n",
    "from onetl.log import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "# Тут важный момент. \n",
    "# Так как мы при поднятии спарк сессии указываем пакеты только один раз \n",
    "# Надо для второго коннектора применить метод maven_packages.extend()\n",
    "maven_packages = Postgres.get_packages()\n",
    "maven_packages.extend(Clickhouse.get_packages())\n",
    "\n",
    "spark = (SparkSession.buider\n",
    "                     .master('local[*]')\n",
    "                     .appName('onETLdemo')\n",
    "                     .config(\"spark.jars.packages\", \",\".join(maven_packages)) # распаковка пакетов\n",
    "                     .config(\"spark.jars.ivySettings\", \"ivysettings.xml\")     # путь к ivysettings.xml обязательно внутри контура МТС\n",
    "                     .getOrCreate())\n",
    "\n",
    "\n",
    "with open(\"config.yaml\") as file:\n",
    "    conf = yaml.safe_load(file)\n",
    "    conf = DictConfig(conf)\n",
    "\n",
    "ch = Clickhouse(\n",
    "    host=conf.ch.host,\n",
    "    port=conf.ch.port,\n",
    "    database=conf.ch.db,\n",
    "    password=conf.ch.passw,\n",
    "    user=conf.ch.user,\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "ch.check()\n",
    "\n",
    "pg = Postgres(\n",
    "    host=conf.pg.host,\n",
    "    port=conf.pg.port,\n",
    "    database=conf.pg.db,\n",
    "    password=conf.pg.passw,\n",
    "    user=conf.pg.user,\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "pg.check()\n",
    "\n",
    "# Читаем таблицу\n",
    "reader = DBReader(\n",
    "    connection=ch,\n",
    "    table=\"system.columns\"\n",
    ")\n",
    "\n",
    "df_ch = reader.run()\n",
    "\n",
    "df = pg.fetch(\"\"\"\n",
    "                 SELECT table_schema, table_name\n",
    "                 FROM information_schema.tables\n",
    "                 WHERE table_schema = 'public'\n",
    "              \"\"\"\n",
    "             )\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Записываем таблицу\n",
    "writer = DBWriter(\n",
    "    connection=pg,\n",
    "    table=\"df_ch.printSchema()\",\n",
    "    options={\"if_exists\": \"replace_entire_table\"}\n",
    ")\n",
    "\n",
    "writer.run(df_ch)\n",
    "\n",
    "df = pg.fetch(\"\"\"\n",
    "                 SELECT table_schema, table_name\n",
    "                 FROM information_schema.tables\n",
    "                 WHERE table_schema = 'public'\n",
    "              \"\"\"\n",
    "             )\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Смотрим что получилось\n",
    "reader = DBReader(\n",
    "    connection=pg,\n",
    "    table=\"public.ch_columns\"\n",
    ")\n",
    "\n",
    "df_pg = reader.run()\n",
    "\n",
    "df_ch.count()\n",
    "df_pg.count()\n",
    "\n",
    "df_ch.printSchema()\n",
    "df_pg.printSchema()\n",
    "\n",
    "pg.execute('DROP TABLE public.ch_columns')\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce478642",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf148642",
   "metadata": {},
   "source": [
    "<a id='part_04'></a>\n",
    "# 4. Объекты подключения и манипуляции данными с использованием структуры DataFrame [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3ed50",
   "metadata": {},
   "source": [
    "<a id='part_41'></a>\n",
    "## 4.1 FileDataFrameConnections [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ffbc9",
   "metadata": {},
   "source": [
    "`spark.df != pandas.df`\n",
    "\n",
    "Основное отличие `spark.DF` от `pandas.DF` в том что Spark использует распределенные вычисления, а Pandas переносит все вычисления на драйвер."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663614b",
   "metadata": {},
   "source": [
    "**Документация:**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_df_connection/`**](https://onetl.readthedocs.io/en/stable/connection/file_df_connection/)\n",
    "\n",
    "Файловые коннекторы поддерживают следующие соединения:\n",
    "- `Local FS`\n",
    "- `HDFS`\n",
    "- `S3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314a3a2",
   "metadata": {},
   "source": [
    "<a id='part_42'></a>\n",
    "## 4.2 LocalFS [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ec246",
   "metadata": {},
   "source": [
    "**Документация:**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_local_fs.html`**](https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_local_fs.html)\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import SparkLocalFS\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .master(\"local\") # это обязательный параметр\n",
    "                     .appName(\"spark-app-name\")\n",
    "                     .getOrCreate()\n",
    "        )\n",
    "local_fs = SparkLocalFS(spark=spark)\n",
    "local_fs.check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9531f9",
   "metadata": {},
   "source": [
    "<a id='part_43'></a>\n",
    "## 4.3 HDFS [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b26582",
   "metadata": {},
   "source": [
    "**Документация:**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_hdfs/index.html`**](https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_hdfs/index.html)\n",
    "\n",
    "Довольно часто использование Hadoop сочетается с использование Kerberos, поэтому в таких случая перед выполнением операций на HDFS нужно пройти Kerberos авторизацию.\n",
    "\n",
    "В случае использования YARN, необходимо передать дополнительные параметры в сессию Spark, узнать какими они могут быть можно в оригинальной документации.\n",
    "\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import SparkHDFS\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session.\n",
    "# Use names \"spark.yarn.access.hadoopFileSystems\", \"spark.yarn.principal\"\n",
    "# and \"spark.yarn.keytab\" for Spark 2\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .option(\n",
    "        \"spark.kerberos.access.hadoopFileSystems\",\n",
    "        \"hdfs://namenode1.domain.com:8020\",\n",
    "    )\n",
    "    .option(\"spark.kerberos.principal\", \"user\")\n",
    "    .option(\"spark.kerberos.keytab\", \"/path/to/keytab\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Create connection\n",
    "hdfs = SparkHDFS(\n",
    "    host=\"namenode1.domain.com\",\n",
    "    cluster=\"rnd-dwh\",\n",
    "    spark=spark,\n",
    ").check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a439ab",
   "metadata": {},
   "source": [
    "<a id='part_44'></a>\n",
    "## 4.4 S3 [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31672a",
   "metadata": {},
   "source": [
    "**Документация:**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_s3/index.html`**](https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_s3/index.html)\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import SparkS3\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Hadoop AWS libraries loaded\n",
    "maven_packages = SparkS3.get_packages(spark_version=\"3.5.5\")\n",
    "# Some packages are not used, but downloading takes a lot of time. Skipping them.\n",
    "\n",
    "\n",
    "# excluded_packages = SparkS3.get_exclude_packages()\n",
    "excluded_packages = [\n",
    "    \"com.google.cloud.bigdataoss:gcs-connector\",\n",
    "    \"org.apache.hadoop:hadoop-aliyun\",\n",
    "    \"org.apache.hadoop:hadoop-azure-datalake\",\n",
    "    \"org.apache.hadoop:hadoop-azure\",\n",
    "]\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .config(\"spark.jars.excludes\", \",\".join(excluded_packages))\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"magic\")\n",
    "    .config(\n",
    "        \"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\",\n",
    "        \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.parquet.output.committer.class\",\n",
    "        \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.sources.commitProtocolClass\",\n",
    "        \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Create connection\n",
    "s3 = SparkS3(\n",
    "    host=\"domain.com\",\n",
    "    protocol=\"http\",\n",
    "    bucket=\"my-bucket\",\n",
    "    access_key=\"ACCESS_KEY\",\n",
    "    secret_key=\"SECRET_KEY\",\n",
    "    extra={\n",
    "        \"path.style.access\": True\n",
    "    }\n",
    "    spark=spark,\n",
    ").check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4089f07",
   "metadata": {},
   "source": [
    "Конструктор коннектора S3 принимает распространенные параметры host, port и сессию spark. Но кроме перечисленных есть и специфические.\n",
    "\n",
    "### Обычные\n",
    "\n",
    "- `host`\n",
    "- `port`\n",
    "- `protocol(https|http)`\n",
    "- `spark`\n",
    "\n",
    "### Специфические\n",
    "\n",
    "- `[access_key]`\n",
    "- `[secret_key]`\n",
    "- `[session_token]`\n",
    "- `[region]`\n",
    "- `bucket`\n",
    "\n",
    "### `s3()` - `extra`\n",
    "\n",
    "**Документация:**\n",
    "\n",
    "[**`https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#General_S3A_Client_configuration`**](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#General_S3A_Client_configuration)\n",
    "\n",
    "`extra` - это словарь описывающий особые опции подключения к S3\n",
    "\n",
    "```python\n",
    "extra={\n",
    "    \"path.style.access\": True,\n",
    "    \"committer.magic.enabled\": True,\n",
    "    \"commiter.name\": \"magic\",\n",
    "    \"connection.timeout\": 300000,\n",
    "}\n",
    "```\n",
    "\n",
    "### S3 - Troubleshooting\n",
    "\n",
    "**Документация:**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_s3/troubleshooting.html`**](https://onetl.readthedocs.io/en/stable/connection/file_df_connection/spark_s3/troubleshooting.html)\n",
    "\n",
    "Для подключения к источнику S3 используется два дополняющих друг друга механизма `Hadoop` и `aws`. Однако так же может приводить к ошибкам. В случае увеличении времени получения информации об ошибках стоит увеличить объем диагностической информации.\n",
    "\n",
    "```python\n",
    "spark.sparkContext.setLogLevel(\"debug\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10c9f3",
   "metadata": {},
   "source": [
    "<a id='part_45'></a>\n",
    "## 4.5 Files Formats [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0ff6f",
   "metadata": {},
   "source": [
    "- `CSV`\n",
    "- `Excel`\n",
    "- `JSON`\n",
    "- `JSONLine` - упрощенная версия JSON\n",
    "- `XML`\n",
    "- `Avro` - bigdata\n",
    "- `ORC` - bigdata\n",
    "- `Parquet` - bigdata\n",
    "\n",
    "\n",
    "\n",
    "### CSV\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/csv.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/csv.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-csv.html`**](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "**Параметры**\n",
    "- `sep` - разделитель значений\n",
    "- `encoding` - кодировка\n",
    "- `quote` - символ экранирования разделителей\n",
    "- `escape` - символ экранирования\n",
    "- `header` - наличие заголовка\n",
    "- `lineSep` - разделитель строк\n",
    "\n",
    "\n",
    "**Методы**\n",
    "- методов у объекта нет\n",
    "\n",
    "\n",
    "### Excel\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/excel.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/excel.html)\n",
    "\n",
    "[**`https://github.com/nightscape/spark-excel`**](https://github.com/nightscape/spark-excel)\n",
    "\n",
    "**Параметры**\n",
    "- `header` - флаг наличия заголовка\n",
    "- `inferSchema` - флаг получения схемы на основе данных источника\n",
    "\n",
    "**Методы**\n",
    "- `get_packages` - возвращает имя актуального пакета для использования в Spark сессии\n",
    "\n",
    "\n",
    "\n",
    "### JSON\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/json.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/json.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-json.html`**](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "\n",
    "**Параметры**\n",
    "- `multiLine` - флаг указывающий на многострочность\n",
    "- `encoding` - кодировка\n",
    "- `lineSep` - разделитель строк\n",
    "\n",
    "**Методы**\n",
    "- методов у объекта нет\n",
    "\n",
    "\n",
    "\n",
    "### JSONLine\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/jsonline.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/jsonline.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-json.html`**](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "\n",
    "**Параметры**\n",
    "- `multiLine` - флаг указывающий на многострочность\n",
    "- `encoding` - кодировка\n",
    "- `lineSep` - разделитель строк\n",
    "\n",
    "**Методы**\n",
    "- методов у объекта нет\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### XML\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/xml.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/xml.html)\n",
    "\n",
    "[**`https://github.com/databricks/spark-xml`**](https://github.com/databricks/spark-xml)\n",
    "\n",
    "**Параметры**\n",
    "- `rowTag` - имя элемента содержащего строки или записи\n",
    "\n",
    "**Методы**\n",
    "- `get_packages` - возвращает имя актуального пакета для использования в Spark сессии\n",
    "\n",
    "\n",
    "\n",
    "### Avro\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/avro.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/avro.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-avro.html`**](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)\n",
    "\n",
    "**Параметры**\n",
    "- `avroSchema` - схема данных\n",
    "- `avroSchemaUrl` - адрес по которому можно получить схему данных\n",
    "\n",
    "**Методы**\n",
    "- `get_packages` - возвращает имя актуального пакета для использования в Spark сессии\n",
    "\n",
    "\n",
    "### ORC\n",
    "\n",
    "На большинстве Hadoop кластеров компании по умолчанию используется формат ORC\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/orc.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/orc.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-orc.html`**](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)\n",
    "\n",
    "**Параметры**\n",
    "- параметров у объекта нет\n",
    "\n",
    "**Методы**\n",
    "- методов у объекта нет\n",
    "\n",
    "\n",
    "\n",
    "### Parquet\n",
    "\n",
    "Формат Parquet часто используется в экосистеме bigdata поскольку в определенных ситуациях он имеет ряд преимуществ перед ORC.\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_formats/parquet.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_formats/parquet.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-parquet.html`**](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n",
    "\n",
    "**Параметры**\n",
    "- параметров у объекта нет\n",
    "\n",
    "**Методы**\n",
    "- методов у объекта нет\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3c0bf",
   "metadata": {},
   "source": [
    "<a id='part_46'></a>\n",
    "## 4.6 FileDFReader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc90a82",
   "metadata": {},
   "source": [
    "Объект чтения данных типа DataFrame\n",
    "\n",
    "Имеет конструктор и метод `run()`\n",
    "\n",
    "- `FileDFReader()`\n",
    "- `run()`\n",
    "\n",
    "### Параметры конструктора\n",
    "\n",
    "- `connection` - объект соединения\n",
    "- `source_path` - путь до директории\n",
    "- `format`\n",
    "\n",
    "### `df_schema`\n",
    "\n",
    "Схема, с помощью которых можно предопределить типы данных считываемого датафрейма\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"_id\", IntegerType()),\n",
    "        StructField(\"text_string\", StringType()),\n",
    "        StructField(\"hwm_int\", IntegerType()),\n",
    "        StructField(\"hwm_datetime\", TimestampType()),\n",
    "        StructField(\"float_value\", DoubleType()),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### `options`\n",
    "\n",
    "Как и у `DBReader` у объекта `FileDF Reader` есть опции\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_df_reader/options.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_df_reader/options.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html`**](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html)\n",
    "\n",
    "### `run()`\n",
    "\n",
    "Метод `run()` как и у объекта `DBReader` читает данные в датафрейм. Обратите внимание что путь к файлу можно передать как в коснтрукторе объекта, так и непосредственно в этом методе. \n",
    "\n",
    "Разница лишь в том, что в объекте мы указываем путь до папки с файлами, а в метод передаем список конкретных файлов или их имена.\n",
    "\n",
    "```python\n",
    "from onetl.connection import SparkLocalFS\n",
    "from onetl.file import FileDFReader\n",
    "from onetl.file.format import CSV\n",
    "\n",
    "csv = CSV(delimiter=\",\")\n",
    "local_fs = SparkLocalFS(spark=spark)\n",
    "\n",
    "reader = FileDFReader(\n",
    "    connection=local_fs,\n",
    "    format=csv,\n",
    "    source_path=\"/path\",\n",
    ")\n",
    "\n",
    "df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60513b",
   "metadata": {},
   "source": [
    "<a id='part_47'></a>\n",
    "## 4.7 FileDFWriter [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0d52d",
   "metadata": {},
   "source": [
    "Объект записи данных типа DataFrame\n",
    "\n",
    "Имеет конструктор и метод `run()`\n",
    "\n",
    "- `FileDFWriter()`\n",
    "- `run(df)`\n",
    "\n",
    "### Параметры конструктора\n",
    "\n",
    "- `connection` - объект соединения\n",
    "- `target_path` - путь до директории куда записать файл\n",
    "- `format`\n",
    "\n",
    "\n",
    "### `options`\n",
    "\n",
    "Как и у `DBWriter` у объекта `FileDFWriter` есть опции\n",
    "\n",
    "**Документация**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/file_df/file_df_writer/options.html`**](https://onetl.readthedocs.io/en/stable/file_df/file_df_writer/options.html)\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html`**](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)\n",
    "\n",
    "У `FileDFWriter` есть только один метод `run(df)` он очень простой, принимает датафрейм.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.connection import SparkLocalFS\n",
    "from onetl.file import FileDFWriter\n",
    "from onetl.file.format import CSV\n",
    "\n",
    "csv = CSV(delimiter=',')\n",
    "local_fs = SparkLocalFS(spark=spark)\n",
    "\n",
    "writer = FileDFWriter(\n",
    "    connection=local_fs,\n",
    "    format=csv,\n",
    "    target_path=\"/path/to/directory\",\n",
    "    options=FileDFWriter.Options(if_exists=\"replace_entire_directory\"),\n",
    ")\n",
    "\n",
    "writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a072a6",
   "metadata": {},
   "source": [
    "<a id='part_48'></a>\n",
    "## 4.8 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65987a55",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import HDFS, S3, SparkLocalFS, SparkS3, SparkHDFS\n",
    "from onetl.file import FileDFReader, FileDFWriter\n",
    "from onetl.log import setup_logging\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from onetl.file.format import CSV, Excel, ORC\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "# посмотрим какие файлы нам доступны \n",
    "!ls - l\n",
    "# Вывод: country_ref.cav\n",
    "\n",
    "# Посмотрим содержимое файла\n",
    "!head -6 country_ref.csv\n",
    "\n",
    "\n",
    "# Чтобы у нас была возможность работать с S3 и форматом Excel подключим внешние пакеты\n",
    "maven_packages = [] # создали пустой список\n",
    "maven_packages.extend(SparkS3.get_packages(spark_version=\"3.4.2\"))\n",
    "maven_packages.extend(Excel.get_packages(spark_version=\"3.4.2\"))\n",
    "# добавили в это список имена пакетов\n",
    "\n",
    "# Создадим спарк сессию\n",
    "\n",
    "spark = (\n",
    "    SparkSession.buider.master(\"local\")\n",
    "    .appName(\"check onETL file DF function\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .config(\"spark.hadoop.fs.s3a.commiter.magic.enabled\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.commiter.name\", \"magic\")\n",
    "    .config(\"spark.rpc.message.maxSize\", 1024)\n",
    "    .config(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\",\n",
    "            \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\",)\n",
    "    .config(\"spark.sql.parquet.output.committer.class\",\n",
    "            \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\",)\n",
    "    .config(\"spark.sql.sources.commitProtocolClass\",\n",
    "            \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\",)\n",
    "    .getOrCreate()    \n",
    ")\n",
    "\n",
    "# Создадим подключение к S3\n",
    "\n",
    "s3 = S3(\n",
    "    host=\"minio\",\n",
    "    protocol=\"http\",\n",
    "    port=9000,\n",
    "    bucket=os.environ[\"MINIO_MY_BUCKET\"],\n",
    "    access_key=os.environ[\"MINIO_ACCESS_KEY\"],\n",
    "    secret_key=os.environ[\"MINIO_SECRET_KEY\"]\n",
    ")\n",
    "\n",
    "s3.check()\n",
    "\n",
    "s3.list_dir(\"/\")\n",
    "\n",
    "# Соединение с поддержкой чтения данных в датафрейм\n",
    "s3 = SparkS3(\n",
    "    host=\"minio\",\n",
    "    protocol=\"http\",\n",
    "    port=9000,\n",
    "    bucket=os.environ[\"MINIO_MY_BUCKET\"],\n",
    "    access_key=os.environ[\"MINIO_ACCESS_KEY\"],\n",
    "    secret_key=os.environ[\"MINIO_SECRET_KEY\"],\n",
    "    extra={\"path.style.access\": True,},\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "s3s.check()\n",
    "\n",
    "# Создадим экземпляр объекта чтения данных FileDFReader\n",
    "\n",
    "reader = FileDFReader(\n",
    "    connection=s3s,\n",
    "    format=Excel(header=True, inferSchema=True),\n",
    "    source_path=\"/\"\n",
    ")\n",
    "\n",
    "df = reader.run([\"country_population.xls\"])\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Создадим объект подключения локальной файловой системы\n",
    "# Чтобы инициализировать его достаточно передать только spark сессию\n",
    "local_fs = SparkLocalFS(spark=spark) \n",
    "\n",
    "reader = FileDFReader(\n",
    "    connection=local_fs,\n",
    "    format=CSV(sep=\";\", header=True),\n",
    "    source_path=\"/opt/endata\",\n",
    ")\n",
    "\n",
    "# Создадим датафрейм который будет соджержать данные из S3 и локальной файловой системы\n",
    "df = df.join(reader.run([\"country_ref.csv\"]), 'country_code')\n",
    "# Объединение данных будет происходить по полю 'country_code'\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Отбросим столбецй с кодом страны, чтобы получить финальный набор данных\n",
    "\n",
    "df = df.drop(\"country_code\")\n",
    "\n",
    "# Объединенные данные мы хотим записать на файловую систему HDFS\n",
    "\n",
    "hdfs = HDFS(host=\"namenode\", port=9870, user=\"hadoop\")\n",
    "\n",
    "hdfs.check()\n",
    "\n",
    "# Посмотрим какие данные есть на HDFS\n",
    "\n",
    "hdfs.list_dir(\"/\")\n",
    "\n",
    "# Создадим папку\n",
    "\n",
    "hdfs.create_dir(\"/country\")\n",
    "\n",
    "hdfs.list_dir(\"/country\")\n",
    "\n",
    "# ЧТобы записать наши данные нам потребуется подключение поддерживающее запись датафрейма\n",
    "# Созадим его\n",
    "\n",
    "shdfs = SparkHDFS(\n",
    "    host=\"namenode\",\n",
    "    port=8020,\n",
    "    spark=spark,\n",
    "    cluster=\"hadoop\"\n",
    ")\n",
    "\n",
    "shdfs.check()\n",
    "\n",
    "# Создадим объект записи данных в файл\n",
    "\n",
    "writer = FileDFWriter(\n",
    "    connection=shdfs,\n",
    "    format=ORC(),\n",
    "    target_path=\"/country/population\"\n",
    ")\n",
    "\n",
    "# Запишем данные\n",
    "\n",
    "writer.run(df)\n",
    "\n",
    "hdfs.list_dir(\"/country\")\n",
    "hdfs.list_dir(\"/country/population\")\n",
    "\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bef4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7039b8",
   "metadata": {},
   "source": [
    "<a id='part_05'></a>\n",
    "# 5. Нестандартные коннекторы: Greenplum, MongoDB, Teradata [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502dafe",
   "metadata": {},
   "source": [
    "<a id='part_51'></a>\n",
    "## 5.1 Введение [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f2c5",
   "metadata": {},
   "source": [
    "Нестандартные коннекторы:\n",
    "\n",
    "- `Hive`\n",
    "- `Greenplum`\n",
    "- `Clickhouse`\n",
    "- `Kafka`\n",
    "- `MongoDB`\n",
    "- `Teradata`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8c2e5",
   "metadata": {},
   "source": [
    "<a id='part_52'></a>\n",
    "## 5.2 Hive [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b1084",
   "metadata": {},
   "source": [
    "### Описание\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/hive/index.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/hive/index.html)\n",
    "\n",
    "Hive представляет собой СУБД которая использует для хранения данных сторонние механизмы, причем одни для хранения собственных данных, а другие для хранилища метаданных. Данные могут храниться например на HDFS или S3, а хранилище метаданных может быть реляционная база данных.\n",
    "\n",
    "У этого коннектора нет метода `fetch()`поскольку в отличие от других в случае Hive библиотека onETL использует только его метаданные, кроме того, в отличии от большинства других коннекторов, у этого объекта нет метода `get_packages()` поскольку Spark по умолчанию включает в себя пакеты необходимые для работы с Hive.\n",
    "\n",
    "И еще у коннектора Hive есть метод `get_current()` инициализирующий соединение с текущим кластером HDFS, однако использовать этот метод можно только в случае привязки соответствующих связей, в нашей компании это доступно с использованием библиотеки `mtspark`\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `Hive()`\n",
    "- `check()`\n",
    "- `get_current()`\n",
    "- `sql()`\n",
    "- `execute()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd48dd7e",
   "metadata": {},
   "source": [
    "### Конструктор Hive\n",
    "\n",
    "Конструктор Hive принимает два парамтера. Имя кластера и Spark сессию. Оба параметра являются обязательными. Обратите внимание что при создании сессии Spark мы активируем поддержку HiveSpark `.enableHiveSupport()`\n",
    "\n",
    "```python\n",
    "from onetl.connection import Hive\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Hive-connector\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark).check()\n",
    "```\n",
    "\n",
    "Параметр | Тип значения | Объяснение\n",
    "-|-|-\n",
    "`spark`|`SparkSession`|Сессия Spark\n",
    "`cluster`|`str`|Имя кластера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0930f7",
   "metadata": {},
   "source": [
    "### Hive+DBReader\n",
    "\n",
    "1. Чтобы работать с данными более чем из одной таблицы, нужно будет прочитать каждую отдельно, а для объединения данных использовать `spark dataframe api`\n",
    "2. DBReader позволдяет использовать стратегию инкрементального чтения данных, а произвольный SQL потребует это делать самостоятельно\n",
    "3. Чтобы получить данные нужно вызвать метод `.run()` объекта `DBReader`, который вернет датафрейм.\n",
    "\n",
    "\n",
    "```python\n",
    "connection = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=connection,\n",
    "    source=\"default.test\",\n",
    "    where=\"d_id > 100\",\n",
    "    columns=[\"d_id\", \"d_name\", \"d_age\"]\n",
    ")\n",
    "\n",
    "df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14240c71",
   "metadata": {},
   "source": [
    "### Hive+DBWriter\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/hive/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/hive/write.html)\n",
    "\n",
    "Запись осуществляется с вызовом метода `.run()` с передачей ему датафрейма, который мы хотим записать.\n",
    "\n",
    "Возможные параметры:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301c395",
   "metadata": {},
   "source": [
    "![](data/onetl_024.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07366a9b",
   "metadata": {},
   "source": [
    "### Hive методы\n",
    "\n",
    "- `execute()` - предназначен для выполнения запросов определения данных DDL. Хотя он и позволяет выполнить некоторые запросы манипуляции данными, рекомендуется ограничится именно DDL\n",
    "- `sql()` - Позволяет выполнить произвольный SQL запрос не используя `spark DataFrame api`. Он принимает единственный параметр: строку запроса, а возвращает датафрейм.\n",
    "\n",
    "```python\n",
    "connection = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "df = connection.sql(\"SELECT * FROM mytable\")\n",
    "connection.execute(\"ALTER TABLE mytable DROP PARTITION(date='2023-02-01')\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb151e",
   "metadata": {},
   "source": [
    "<a id='part_53'></a>\n",
    "## 5.3 Greenplum [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ca1e5",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/index.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/index.html)\n",
    "\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `Greenplum()`\n",
    "- `get_packages()`\n",
    "- `check()`\n",
    "- `execute()`\n",
    "- `fetch()`\n",
    "- `close()`\n",
    "\n",
    "### Описание коннектора\n",
    "\n",
    "[**`https://techdocs.broadcom.com/us/en/vmware-tanzu/data-solutions/tanzu-greenplum-connector-for-apache-spark/2-3/gp-connector-spark/release_notes.html`**](https://techdocs.broadcom.com/us/en/vmware-tanzu/data-solutions/tanzu-greenplum-connector-for-apache-spark/2-3/gp-connector-spark/release_notes.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72913e1b",
   "metadata": {},
   "source": [
    "![](data/onetl_025.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283aeefc",
   "metadata": {},
   "source": [
    "**Порядок действий**\n",
    "\n",
    "1. **Установка соединения** - Сначала драйвер spark устанавливает соединение с мастером Greenplum\n",
    "2. **Запрос данных** - Затем экзекьютеры spark запрашивают данные \n",
    "3. **Распределение задач**  \n",
    "4. **Возврат данных** - которые возвращаются им посредством сегментов Greenplum\n",
    "5. **Данные получены** - и в завершение процесса объединяются на драйвере\n",
    "6. **Завершение соединения**\n",
    "\n",
    "\n",
    "Перед запуском процесса мы должны обеспечить беспрепядственную сетевую связанность между узлами сегмента Greenplum и хотсами на которых выполняются экзекьюторы. В силу большого количества нюансов, настройка окружения для таких процессов требует значительного внимания и по этому в документации есть специальный раздел посвященный предвариельным условиям и рекомендациям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbfd9a",
   "metadata": {},
   "source": [
    "### Конструктор Greenplum\n",
    "\n",
    "[**`https://github.com/pgjdbc/pgjdbc#connection-properties`**](https://github.com/pgjdbc/pgjdbc#connection-properties)\n",
    "\n",
    "[**`https://techdocs.broadcom.com/us/en/vmware-tanzu/data-solutions/tanzu-greenplum-connector-for-apache-spark/2-3/gp-connector-spark/options.html`**](https://techdocs.broadcom.com/us/en/vmware-tanzu/data-solutions/tanzu-greenplum-connector-for-apache-spark/2-3/gp-connector-spark/options.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee7ddc",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Greenplum\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Greenplum connector loaded\n",
    "maven_packages = Greenplum.get_packages(spark_version=\"3.2\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .config(\"spark.executor.allowSparkContext\", \"true\")\n",
    "    # IMPORTANT!!!\n",
    "    # Set number of executors according to \"Prerequisites\" -> \"Number of executors\"\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 10)\n",
    "    .config(\"spark.executor.cores\", 1)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# IMPORTANT!!!\n",
    "# Set port range of executors according to \"Prerequisites\" -> \"Network ports\"\n",
    "extra = {\n",
    "    \"server.port\": \"41000-42000\",\n",
    "}\n",
    "\n",
    "# Create connection\n",
    "greenplum = Greenplum(\n",
    "    host=\"master.host.or.ip\",\n",
    "    user=\"user\",\n",
    "    password=\"*****\",\n",
    "    database=\"target_database\",\n",
    "    extra=extra,\n",
    "    spark=spark,\n",
    ").check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf9f85",
   "metadata": {},
   "source": [
    "![](data/onetl_026.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef5ad8",
   "metadata": {},
   "source": [
    "### Greenplum + DBReader\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/read.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/read.html)\n",
    "\n",
    "Использование DBReader поволяет в полном объеме задействовать параллелизм Spark и метки инкрементальных процессов HWM. \n",
    "\n",
    "Отметим что исходя из имеющегося опыта использования Greenplum, не стоит с помощью Spark читать данные из представлений (View) и джойнов (JOIN) это приводит к неэффективному использованию ресурсов\n",
    "\n",
    "В примере ниже есть пример использования специфичяеских для Greenplum опций чтения\n",
    "\n",
    "```python\n",
    "from onetl.connection import Greenplum\n",
    "from onetl.db import DBReader\n",
    "\n",
    "greenplum = Greenplum(...)\n",
    "\n",
    "read_options = Greenplum.ReadOptions(\n",
    "    partition_column=\"reg_id\",\n",
    "    num_partitions=10,\n",
    ")\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=greenplum,\n",
    "    source=\"schema.table\",\n",
    "    columns=[\"id\", \"key\", \"CAST(value AS string) value\", \"updated_dt\"],\n",
    "    where=\"key = 'something'\",\n",
    "    options=read_options\n",
    ")\n",
    "df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501723e",
   "metadata": {},
   "source": [
    "### Greenplum + DBWriter\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/greenplum/write.html)\n",
    "\n",
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.connection import Greenplum\n",
    "from onetl.db import DBWriter\n",
    "\n",
    "greenplum = Greenplum(...)\n",
    "\n",
    "df = ...  # data is here\n",
    "\n",
    "write_options = Greenplum.WriteOptions(\n",
    "        if_exists=\"append\",\n",
    "        # by default distribution is random\n",
    "        distributedBy=\"mycolumn\",\n",
    "        # partitionBy is not supported\n",
    "        truncate=False,\n",
    "    )\n",
    "\n",
    "writer = DBWriter(\n",
    "    connection=greenplum,\n",
    "    target=\"schema.table\",\n",
    "    options=write_options,\n",
    ")\n",
    "\n",
    "writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6f108",
   "metadata": {},
   "source": [
    "![](data/onetl_027.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed23a8d",
   "metadata": {},
   "source": [
    "### Greenplum методы\n",
    "\n",
    "- `execute()` - позволяет выполнить DDL и DML запросы. Но рекомендуется только для DDL\n",
    "- `fetch()` - метод для извлечения небольших наборов данных, он использует JBDC драйвер Postgres и поэтому возврат некоторых типов данных может отличаться от нативного драйвера Greenplum\n",
    "- `close()` \n",
    "\n",
    "```python\n",
    "connection.execute(\"CREATE TABLE target_table(id NUMBER, data VARCHAR)\")\n",
    "\n",
    "df = connection.fetch(\"SELECT * FROM mytable\", {\"fetchsize\": 10000})\n",
    "\n",
    "connection.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8775e",
   "metadata": {},
   "source": [
    "<a id='part_54'></a>\n",
    "## 5.4 Clickhouse [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910f5a9",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/connection.html)\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `Clickhouse()`\n",
    "- `get_packages()`\n",
    "- `check()`\n",
    "- `sql()`\n",
    "- `execute()`\n",
    "- `fetch()`\n",
    "- `close()`\n",
    "\n",
    "### Конструктор Clickhouse\n",
    "\n",
    "[**`https://github.com/ClickHouse/clickhouse-java/tree/main/clickhouse-jdbc#configuration`**](https://github.com/ClickHouse/clickhouse-java/tree/main/clickhouse-jdbc#configuration)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/connection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f449ce",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Clickhouse\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Clickhouse driver loaded\n",
    "maven_packages = Clickhouse.get_packages()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Create connection\n",
    "clickhouse = Clickhouse(\n",
    "    host=\"database.host.or.ip\",\n",
    "    user=\"user\",\n",
    "    password=\"*****\",\n",
    "    extra={\"continueBatchOnError\": \"false\"},\n",
    "    spark=spark,\n",
    ").check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df4c41",
   "metadata": {},
   "source": [
    "![](data/onetl_028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2c6af",
   "metadata": {},
   "source": [
    "### Clickhouse + DBReader\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html`**](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/0.10.2/connection/db_connection/clickhouse/read.html`**](https://onetl.readthedocs.io/en/0.10.2/connection/db_connection/clickhouse/read.html)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/0.10.2/connection/db_connection/clickhouse/types.html#clickhouse-types`**](https://onetl.readthedocs.io/en/0.10.2/connection/db_connection/clickhouse/types.html#clickhouse-types)\n",
    "\n",
    "При чтении из Clickhouse может возникнуть проблема связанная с тем, что в Clickhouse есть типы данных которые не поддерживаются напрямую Spark. Для этого есть страница с маппингом данных `Типов Clickhouse` на `Типы Spark`\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.connection import Clickhouse\n",
    "from onetl.db import DBReader\n",
    "\n",
    "clickhouse = Clickhouse(...)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=clickhouse,\n",
    "    source=\"schema.table\",\n",
    "    columns=[\"id\", \"key\", \"CAST(value AS String) value\", \"updated_dt\"],\n",
    "    where=\"key = 'something'\",\n",
    "    options=Clickhouse.ReadOptions(partition_column=\"id\", num_partitions=10),\n",
    ")\n",
    "\n",
    "df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953308e",
   "metadata": {},
   "source": [
    "### Clichhouse + DBWriter\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/clickhouse/write.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac55bb",
   "metadata": {},
   "source": [
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.connection import Clickhouse\n",
    "from onetl.db import DBWriter\n",
    "\n",
    "clickhouse = Clickhouse(...)\n",
    "\n",
    "df = ...  # data is here\n",
    "\n",
    "writer = DBWriter(\n",
    "    connection=clickhouse,\n",
    "    target=\"schema.table\",\n",
    "    options=Clickhouse.WriteOptions(\n",
    "        if_exists=\"append\",\n",
    "        # ENGINE is required by Clickhouse\n",
    "        createTableOptions=\"ENGINE = MergeTree() ORDER BY id\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8524bf",
   "metadata": {},
   "source": [
    "![](data/onetl_029.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa2c6a",
   "metadata": {},
   "source": [
    "### Clickhouse методы\n",
    "\n",
    "- `execute()`\n",
    "- `fetch()`\n",
    "- `sql()`\n",
    "- `close()`\n",
    "\n",
    "Примеры использования:\n",
    "\n",
    "```python\n",
    "\n",
    "# Первый пример\n",
    "\n",
    "clickhouse = Clickhouse( ... )\n",
    "\n",
    "df = clickhouse.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        key,\n",
    "        CAST(value AS string) value,\n",
    "        updated_at\n",
    "    FROM some.mytable\n",
    "    WHERE key = 'something'\n",
    "    \"\"\",\n",
    "    options=Clickhouse.ReadOptions(\n",
    "        partition_column=\"id\",\n",
    "        num_partitions=10,\n",
    "        lower_bound=0,\n",
    "        upper_bound=1000,\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "clickhouse.close()\n",
    "\n",
    "# Второй пример\n",
    "\n",
    "with connectioт:\n",
    "    connection.execute(\"CREATE TABLE mytable(id NUMBER, data VARCHAR)\")\n",
    "    connection.execute(\"INSERT INTO mytable VALUES (1, 'test record')\", {\"isolationLevel\": \"READ_COMMITTED\"})\n",
    "...\n",
    "df = connection.fetch(\"SELECT * FROM mytable\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df826b",
   "metadata": {},
   "source": [
    "<a id='part_55'></a>\n",
    "## 5.5 Kafka [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a22d5",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/connection.html)\n",
    "\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `Kafka()`\n",
    "- `get_packages()`\n",
    "- `check()`\n",
    "- `close()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb38d4",
   "metadata": {},
   "source": [
    "```python\n",
    "kafka = Kafka(\n",
    "    addresses=[\"mybroker:9092\", \"anotherbroker:9092\"],\n",
    "    cluster=\"my-cluster\",\n",
    "    protocol=Kafka.SSLProtocol(\n",
    "        keystore_type=\"PEM\",\n",
    "        keystore_certificate_chain=Path(\"path/to/user.crt\").read_text(),\n",
    "        keystore_key=Path(\"path/to/user.key\").read_text(),\n",
    "        truststore_type=\"PEM\",\n",
    "        truststore_certificates=Path(\"/path/to/server.crt\").read_text(),\n",
    "    ),\n",
    "    auth=Kafka.ScramAuth(\n",
    "        user=\"me\",\n",
    "        password=\"abc\",\n",
    "        digest=\"SHA-512\",\n",
    "    ),\n",
    "    spark=spark,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f6148",
   "metadata": {},
   "source": [
    "### Kafka Protocol\n",
    "\n",
    "**PlaintextProtocol**\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/plaintext_protocol.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/plaintext_protocol.html)\n",
    "\n",
    "\n",
    "**Реализации:**\n",
    "- `PlaintextProtocol`\n",
    "- `SSLProtocol`\n",
    "\n",
    "**Методы:**\n",
    "- `get_options()` - возвращает словрь с опциями, которые предусмотрены протоколом\n",
    "- `cleanup()` - используется при завершении соединения с брокером\n",
    "\n",
    "### Kafka SSLProtocol\n",
    "\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/ssl_protocol.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/ssl_protocol.html)\n",
    "\n",
    "Использование этого протокола требует указания перечисленных параметров:\n",
    "\n",
    "- `keystore_type`\n",
    "- `keystore_location`\n",
    "- `keystore_password`\n",
    "- `keystore_certificate_chain`\n",
    "- `keystore_key`\n",
    "- `key_password`\n",
    "- `truststore_type`\n",
    "- `truststore_location`\n",
    "- `truststore_password`\n",
    "- `truststore_certificates`\n",
    "\n",
    "\n",
    "### Kafka Auth\n",
    "\n",
    "Коннектор Kafka поддерживает несколько вариантов авторизации, для каждого из них свой набор параметров:\n",
    "\n",
    "- `Basic` - [**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/basic_auth.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/basic_auth.html)\n",
    "- `Scram` - [**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/scram_auth.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/scram_auth.html)\n",
    "- `Kerberos` - [**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/kerberos_auth.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/kerberos_auth.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Реализации:**\n",
    "- `BasicAuth`\n",
    "- `KerberosAuth`\n",
    "- `ScramAuth`\n",
    "\n",
    "**Методы:**\n",
    "- `get_jaas_conf()` - возвращает конфигурацию по спецификации сервиса авторизации идентификации JAVA\n",
    "- `get_options()` - возвращает словрь с опциями, которые предусмотрены протоколом\n",
    "- `cleanup()` - используется при завершении соединения с брокером"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd068fb",
   "metadata": {},
   "source": [
    "### Kafka + DBReader\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html`**](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/read.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/read.html)\n",
    "\n",
    "Читаемые данные из Kafka должны иметь предопределенную структуру и не могут быть определены автоматически."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b87e8",
   "metadata": {},
   "source": [
    "![](data/onetl_030.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed0e9e",
   "metadata": {},
   "source": [
    "### Kafka + DBWriter\n",
    "\n",
    "[**`https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html`**](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/kafka/write.html)\n",
    "\n",
    "Как и в случае с чтением, при записи в Kafka, структура данных должна быть четко предопределена.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a3277",
   "metadata": {},
   "source": [
    "![](data/onetl_031.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb27ee",
   "metadata": {},
   "source": [
    "<a id='part_56'></a>\n",
    "## 5.6 MongoDB [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705286dd",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/connection.html)\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `MongoDB()`\n",
    "- `get_packages()`\n",
    "- `check()`\n",
    "- `pipeline()` - аналог метода .sql()\n",
    "\n",
    "### Конструктор\n",
    "\n",
    "[**`https://www.mongodb.com/docs/manual/reference/connection-string/#std-label-connections-connection-options`**](https://www.mongodb.com/docs/manual/reference/connection-string/#std-label-connections-connection-options)\n",
    "\n",
    "```python\n",
    "from onetl.connection import MongoDB\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with MongoDB connector loaded\n",
    "maven_packages = MongoDB.get_packages(spark_version=\"3.4\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Create connection\n",
    "mongo = MongoDB(\n",
    "    host=\"master.host.or.ip\",\n",
    "    user=\"user\",\n",
    "    password=\"*****\",\n",
    "    database=\"target_database\",\n",
    "    spark=spark,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32df6d0",
   "metadata": {},
   "source": [
    "![](data/onetl_032.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519ec54",
   "metadata": {},
   "source": [
    "### MongoDB + DBReader\n",
    "\n",
    "[**`https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-read-config/`**](https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-read-config/)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/read.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/read.html)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.connection import MongoDB\n",
    "from onetl.db import DBReader\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "mongodb = MongoDB(...)\n",
    "\n",
    "# mandatory\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"_id\", StringType()),\n",
    "        StructField(\"some\", StringType()),\n",
    "        StructField(\n",
    "            \"field\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"nested\", IntegerType()),\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\"updated_dt\", TimestampType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=mongodb,\n",
    "    source=\"some_collection\",\n",
    "    df_schema=df_schema,\n",
    "    where={\"field\": {\"$eq\": 123}},\n",
    "    hint={\"field\": 1},\n",
    "    options=MongoDBReadOptions(batchSize=10000),\n",
    ")\n",
    "df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b143cae",
   "metadata": {},
   "source": [
    "### MongoDB + DBWriter\n",
    "\n",
    "\n",
    "[**`https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-write-config/`**](https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-write-config/)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/write.html)\n",
    "\n",
    "```python\n",
    "from onetl.connection import MongoDB\n",
    "from onetl.db import DBWriter\n",
    "\n",
    "mongodb = MongoDB(...)\n",
    "\n",
    "df = ...  # data is here\n",
    "\n",
    "writer = DBWriter(\n",
    "    connection=mongodb,\n",
    "    target=\"schema.table\",\n",
    "    options=MongoDB.WriteOptions(\n",
    "        if_exists=\"append\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d4ea",
   "metadata": {},
   "source": [
    "![](data/onetl_033.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba9efc",
   "metadata": {},
   "source": [
    "### MongoDB: pipeline()\n",
    "\n",
    "[**`https://www.mongodb.com/docs/manual/core/aggregation-pipeline/`**](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/)\n",
    "\n",
    "[**`https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-read-config/`**](https://www.mongodb.com/docs/spark-connector/current/batch-mode/batch-read-config/)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/pipeline.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/mongodb/pipeline.html)\n",
    "\n",
    "`.pipeline()` это метод который позволяет извлекать данные.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"_id\", StringType()),\n",
    "        StructField(\"some_string\", StringType()),\n",
    "        StructField(\"some_int\", IntegerType()),\n",
    "        StructField(\"some_datetime\", TimestampType()),\n",
    "        StructField(\"some_float\", DoubleType()),\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = connection.pipeline(\n",
    "    collection=\"collection_name\",\n",
    "    df_schema=df_schema,\n",
    "    pipeline={\"$match\": {\"some_int\": {\"$gt\": 999}}},\n",
    "    options=MongoDB.PipelineOptions(hint={\"_id\": 1})\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12af19",
   "metadata": {},
   "source": [
    "![](data/onetl_034.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd011d",
   "metadata": {},
   "source": [
    "<a id='part_57'></a>\n",
    "## 5.7 Teradata [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4a974",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/index.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/index.html)\n",
    "\n",
    "### Методы\n",
    "\n",
    "- `Teradata()`\n",
    "- `get_packages()`\n",
    "- `check()`\n",
    "- `sql()`\n",
    "- `fetch()`\n",
    "- `execute()`\n",
    "- `close()`\n",
    "\n",
    "### Конструктор Teradata\n",
    "\n",
    "[**`https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BABJIHBJ`**](https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BABJIHBJ)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/connection.html)\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import Teradata\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Teradata driver loaded\n",
    "maven_packages = Teradata.get_packages()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-app-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Create connection\n",
    "teradata = Teradata(\n",
    "    host=\"database.host.or.ip\",\n",
    "    user=\"user\",\n",
    "    password=\"*****\",\n",
    "    extra={\n",
    "        \"TMODE\": \"TERA\",  # \"TERA\" or \"ANSI\"\n",
    "        \"LOGMECH\": \"LDAP\",\n",
    "        \"LOG\": \"TIMING\",  # increase log level\n",
    "    },\n",
    "    spark=spark,\n",
    ").check()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e0f31",
   "metadata": {},
   "source": [
    "![](data/onetl_035.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc07dcb",
   "metadata": {},
   "source": [
    "### Teradata + DBReader\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/read.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/read.html)\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import Teradata\n",
    "from onetl.db import DBReader\n",
    "\n",
    "teradata = Teradata(...)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=teradata,\n",
    "    source=\"database.table\",\n",
    "    columns=[\"id\", \"key\", \"CAST(value AS VARCHAR) value\", \"updated_dt\"],\n",
    "    where=\"key = 'something'\",\n",
    "    options=Teradata.ReadOptions(\n",
    "        partitioning_mode=\"hash\",\n",
    "        partitionColumn=\"id\",\n",
    "        numPartitions=10,\n",
    "    ),\n",
    ")\n",
    "df = reader.run()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa70ed",
   "metadata": {},
   "source": [
    "### Teradata + DBWriter\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/write.html`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/write.html)\n",
    "\n",
    "При записи в Teradata есть специфика. Во первых рекомендуется заранее создавать таблицу. Во вторых запись в Teradata возможен только в один поток.\n",
    "\n",
    "```python\n",
    "from onetl.connection import Teradata\n",
    "from onetl.db import DBWriter\n",
    "\n",
    "teradata = Teradata(\n",
    "    ...,\n",
    "    extra={\"TYPE\": \"FASTLOAD\", \"TMODE\": \"TERA\"},\n",
    ")\n",
    "\n",
    "df = ...  # data is here\n",
    "\n",
    "writer = DBWriter(\n",
    "    connection=teradata,\n",
    "    target=\"database.table\",\n",
    "    options=Teradata.WriteOptions(\n",
    "        if_exists=\"append\",\n",
    "        # avoid creating SET table, use MULTISET\n",
    "        createTableOptions=\"NO PRIMARY INDEX\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "writer.run(df.repartition(1))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3146f",
   "metadata": {},
   "source": [
    "![](data/onetl_036.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8efd2",
   "metadata": {},
   "source": [
    "### Terada методы\n",
    "\n",
    "\n",
    "- [**`execute`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/execute.html#onetl.connection.db_connection.teradata.options.TeradataExecuteOptions)\n",
    "- [**`fetch`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/execute.html#onetl.connection.db_connection.teradata.options.TeradataFetchOptions)\n",
    "- [**`sql`**](https://onetl.readthedocs.io/en/stable/connection/db_connection/teradata/sql.html#onetl.connection.db_connection.teradata.options.TeradataSQLOptions)\n",
    "- `close()`\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "df = connection.sql(\"SELECT * FROM mytable\")\n",
    "df = teradata.fetch(\n",
    "    \"SELECT value FROM some.table WHERE key = 'some_value'\",\n",
    "    options=Teradata.FetchOptions(query_timeout=10),\n",
    ")\n",
    "teradata.execute(\"DROP TABLE database.table\")\n",
    "connection.close()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812df9b",
   "metadata": {},
   "source": [
    "<a id='part_58'></a>\n",
    "## 5.8 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591eb68",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from onetl.log import setup_logging\n",
    "from onetl.connection import Hive, MongoDB, Kafka\n",
    "from onetl.db import DBWriter, DBReader\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "packages = MongoDB.get_packages(spark_version=\"3.4.2\")\n",
    "packages.extend(Kafka.get_packages(spark_version=\"3.4.2\"))\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    ".appName(\"test\") \\\n",
    ".config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    ".config(\"spark.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    ".config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/warehouse\") \\\n",
    ".config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "\n",
    "hive = Hive(cluster=\"test\", spark=spark)\n",
    "hive.check()\n",
    "\n",
    "reader = DBReader(connection=hive, table=\"hive_at_hdfs.country_ref\")\n",
    "hive_df = reader.run()\n",
    "\n",
    "hive_df.printSchema()\n",
    "hive_df.count()\n",
    "hive_df.show()\n",
    "\n",
    "# Работа с Kafka порождает сильно больше логов чем работа с другими системами\n",
    "# По этому давайте ограничим уровень логирования только ERROR значениями\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "kafka = Kafka(addresses=[\"kafka:9092\"], cluster=\"test\", spark=spark)\n",
    "\n",
    "kafka.check()\n",
    "\n",
    "reader = DBReader(connection=kafka, table=\"country_ref\")\n",
    "\n",
    "kafka_df = reader.run()\n",
    "\n",
    "kafka_df.printSchema()\n",
    "\n",
    "kafka_df.count()\n",
    "\n",
    "writer = DBWriter(connection=kafka, table=\"country_ref\")\n",
    "\n",
    "writer.run(hive_df)\n",
    "\n",
    "# При выполнении этой операции код упал, и в демо был показан этап поиска ошибки\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fae79",
   "metadata": {},
   "source": [
    "![](data/onetl_037.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde4401",
   "metadata": {},
   "source": [
    "Дело оказалось в структуре данных, с которой требуется осуществить запись в Kafka. Чтобы запись не падала, требуется преобразовать структуру."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff338c12",
   "metadata": {},
   "source": [
    "```python\n",
    "writer.run(hive_df.withColumnRenamed(\"country_code\", \"key\") \\\n",
    "                  .withColumnRenamed(\"country_name\", \"value\"))\n",
    "\n",
    "kafka_df.count()\n",
    "kafka.printSchema()\n",
    "kafka_df.show()\n",
    "\n",
    "from pyspark.sql.functions import decode\n",
    "\n",
    "kafka_df = kafka_df.select(decode(\"key\", \"UTF-8\").alias(\"country_code\"),\n",
    "                           decode(\"value\", \"UTF-8\").alias(\"country_name\"))\n",
    "\n",
    "kafka_df.show(5)\n",
    "\n",
    "mongo = MongoDB(\n",
    "    host=\"mongodb\",\n",
    "    user=os.environ[\"MONGODB_USERNAME\"],\n",
    "    password=os.environ[\"MONGODB_PASSWORD\"],\n",
    "    database=os.environ[\"MONGODB_DATABASE\"],\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "mongo.check()\n",
    "\n",
    "writer = DBWriter(connection=mongo,\n",
    "                  table=\"countru_ref\")\n",
    "\n",
    "writer.run(kafka_df)\n",
    "\n",
    "reader = DBReader(connection=mongo,\n",
    "                  table=\"country_ref\")\n",
    "\n",
    "# Тут код опять падает\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d234b58",
   "metadata": {},
   "source": [
    "![](data/onetl_038.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ef2dc",
   "metadata": {},
   "source": [
    "Дело в том что для этого хранилища обязательным параметром является схема данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fabd12",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType\n",
    ")\n",
    "\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"country_code\", StringType()),\n",
    "        StructField(\"country_name\", StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "reader = DBReader(connection=mongo,\n",
    "                  table=\"country_ref\",\n",
    "                  df_schema=df_schema)\n",
    "\n",
    "mongo_df = reader.run()\n",
    "mongo_df.count()\n",
    "mongo_df.printSchema()\n",
    "mongo_df.show(5)\n",
    "kafka.close()\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911aa22c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af9cf8",
   "metadata": {},
   "source": [
    "<a id='part_06'></a>\n",
    "# 6. Коннекторы к файловым системам [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578a002",
   "metadata": {},
   "source": [
    "<a id='part_61'></a>\n",
    "## 6.1 Введение [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58464baa",
   "metadata": {},
   "source": [
    "Коннекторы к ФС это объекты позволяющие подключачться к различным файловым хранилищам и выполнять операции с файлами и папками директории. Эти коннекторы предназначены прежде всего для операций над файлами системы, а не над данными хранящиеся в них.\n",
    "\n",
    "Данные коннекторы никак не используют Spark и не зависят от него."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd0827",
   "metadata": {},
   "source": [
    "### Поддерживаемые системы\n",
    "- `HDFS`\n",
    "- `FTP`\n",
    "- `FTPS`\n",
    "- `SDFT`\n",
    "- `S3`\n",
    "- `WedDAV`\n",
    "- `Smaba`\n",
    "\n",
    "### Установка\n",
    "\n",
    "```bash\n",
    "pip install onetl[files]\n",
    "```\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_connection/index.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7351a8",
   "metadata": {},
   "source": [
    "<a id='part_62'></a>\n",
    "## 6.2 Возможности коннекторов [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac38e2",
   "metadata": {},
   "source": [
    "### Методы файловых коннекторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f2d35",
   "metadata": {},
   "source": [
    "![](data/onetl_015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb98d82",
   "metadata": {},
   "source": [
    "<a id='part_63'></a>\n",
    "## 6.3 Обзор коннекторов [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d55e0e",
   "metadata": {},
   "source": [
    "### HDFS\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_connection/hdfs/connection.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/hdfs/connection.html)\n",
    "\n",
    "```python\n",
    "from onetl.connection import HDFS\n",
    "\n",
    "hdfs = HDFS(\n",
    "    host=\"namenode1.domain.com\",\n",
    "    user=\"someuser\",\n",
    "    password=\"******\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Параметры конструктора HDFS\n",
    "\n",
    "При использованиии библиотеки `mtspark` большинство из этих параметров становятся необязательными\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01faafa4",
   "metadata": {},
   "source": [
    "![](data/onetl_016.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab578e7",
   "metadata": {},
   "source": [
    "### FTP, FTPS, SFTP\n",
    "\n",
    "- [**`https://onetl.readthedocs.io/en/stable/connection/file_connection/ftp.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/ftp.html)\n",
    "- [**`https://onetl.readthedocs.io/en/stable/connection/file_connection/ftps.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/ftps.html)\n",
    "- [**`https://onetl.readthedocs.io/en/stable/connection/file_connection/sftp.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/sftp.html)\n",
    "\n",
    "\n",
    "```python\n",
    "# FTP\n",
    "from onetl.connection import FTP\n",
    "\n",
    "ftp = FTP(\n",
    "    host=\"ftp.domain.com\",\n",
    "    user=\"someuser\",\n",
    "    password=\"******\",\n",
    ")\n",
    "\n",
    "\n",
    "# FTPS\n",
    "from onetl.connection import FTPS\n",
    "\n",
    "ftps = FTPS(\n",
    "    host=\"ftps.domain.com\",\n",
    "    user=\"someuser\",\n",
    "    password=\"******\",\n",
    ")\n",
    "\n",
    "# SFTP\n",
    "from onetl.connection import SFTP\n",
    "\n",
    "sftp = SFTP(\n",
    "    host=\"192.168.1.19\",\n",
    "    user=\"someuser\",\n",
    "    password=\"******\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Параметры конструктора `[S]FTP[S]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732458e",
   "metadata": {},
   "source": [
    "![](data/onetl_017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa3d04",
   "metadata": {},
   "source": [
    "### WebDAV\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_connection/webdav.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/webdav.html)\n",
    "\n",
    "```python\n",
    "from onetl.connection import WebDAV\n",
    "\n",
    "wd = WebDAV(\n",
    "    host=\"webdav.domain.com\",\n",
    "    user=\"someuser\",\n",
    "    password=\"******\",\n",
    "    protocol=\"https\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Параметры конструктора WebDAV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31936237",
   "metadata": {},
   "source": [
    "![](data/onetl_018.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de68cd",
   "metadata": {},
   "source": [
    "### S3\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_connection/s3.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/s3.html)\n",
    "\n",
    "```python\n",
    "from onetl.connection import S3\n",
    "\n",
    "s3 = S3(\n",
    "    host=\"s3.domain.com\",\n",
    "    protocol=\"http\",\n",
    "    bucket=\"my-bucket\",\n",
    "    access_key=\"ACCESS_KEY\",\n",
    "    secret_key=\"SECRET_KEY\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Параметры конструктора S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74df38",
   "metadata": {},
   "source": [
    "![](data/onetl_019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cae11",
   "metadata": {},
   "source": [
    "### Samba\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/connection/file_connection/samba.html`**](https://onetl.readthedocs.io/en/stable/connection/file_connection/samba.html)\n",
    "\n",
    "```python\n",
    "from onetl.connection import Samba\n",
    "\n",
    "samba = Samba(\n",
    "    host=\"mydomain.com\",\n",
    "    share=\"share_name\",\n",
    "    protocol=\"SMB\",\n",
    "    port=445,\n",
    "    user=\"user\",\n",
    "    password=\"******\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Параметры конструктора Samba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023813c2",
   "metadata": {},
   "source": [
    "![](data/onetl_020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6738d0",
   "metadata": {},
   "source": [
    "### Методы файловых коннекторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082a133",
   "metadata": {},
   "source": [
    "![](data/onetl_015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa475e7",
   "metadata": {},
   "source": [
    "### Отличия от объектов манипуляции файлами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5aabc5",
   "metadata": {},
   "source": [
    "![](data/onetl_021.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284e5ee",
   "metadata": {},
   "source": [
    "<a id='part_64'></a>\n",
    "## 6.4 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc48e6",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from onetl.log import setup_logging\n",
    "from onetl.connection import WebDAV\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "wd = WebDAV(\n",
    "    host=\"wd.mts.ru\",\n",
    "    user=\"user\",\n",
    "    password=\"******\",\n",
    "    protocol=\"https\",\n",
    ")\n",
    "\n",
    "wd.check() # проверить соединение\n",
    "\n",
    "wd.list_dir(\"/\") # посмотреть список файлов в папке\n",
    "\n",
    "wd.get_stat(\"/path/file.csv\") # посмотреть свойство файла\n",
    "\n",
    "wd.download_file(\"/downloading_filename.csv\", \"downloaded_filename.csv\") \n",
    "# скачиваем файл, указываем имя  скачиваемого файла и под которым записать на диск\n",
    "\n",
    "wd.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659b28b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c745fb",
   "metadata": {},
   "source": [
    "<a id='part_07'></a>\n",
    "# 7. Объект скачивания данных FileDownloader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3641a",
   "metadata": {},
   "source": [
    "<a id='part_71'></a>\n",
    "## 7.1 Объекты манипуляции данными [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8f0ce",
   "metadata": {},
   "source": [
    "Объекты манипуляции данными позволяют производить массовые операции над файлами.\n",
    "\n",
    "**Основные:**\n",
    "\n",
    "- `FileDownloader`\n",
    "- `FileUploader`\n",
    "- `FileMover`\n",
    "\n",
    "**Вспомогательные**\n",
    "\n",
    "- `FileFilters` - для отбора файлов по определенному критерию\n",
    "- `FileLimits` - Критерий для огарничения количество обрабатываемых файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18117ff6",
   "metadata": {},
   "source": [
    "<a id='part_72'></a>\n",
    "## 7.2 FileDownloader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164723c0",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_downloader/`**](https://onetl.readthedocs.io/en/stable/file/file_downloader/)\n",
    "\n",
    "### Основные элементы конструкции\n",
    "\n",
    "- `FileDownloader()`\n",
    "- `File Downloader Options`\n",
    "- `File Downloader Results`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee740d",
   "metadata": {},
   "source": [
    "<a id='part_73'></a>\n",
    "## 7.3 Конструктор FileDownloader() [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b1eec",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_downloader/file_downloader.html`**](https://onetl.readthedocs.io/en/stable/file/file_downloader/file_downloader.html)\n",
    "\n",
    "**Пример кода:**\n",
    "\n",
    "```python\n",
    "from onetl.connection import SFTP\n",
    "from onetl.file import FileDownloader\n",
    "\n",
    "sftp = SFTP(...)\n",
    "\n",
    "# create downloader\n",
    "downloader = FileDownloader(\n",
    "    connection=sftp,\n",
    "    source_path=\"/path/to/remote/source\",\n",
    "    local_path=\"/path/to/local\",\n",
    ")\n",
    "\n",
    "# download files to \"/path/to/local\"\n",
    "downloader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436f4c0",
   "metadata": {},
   "source": [
    "![](data/onetl_039.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9dbf4",
   "metadata": {},
   "source": [
    "<a id='part_74'></a>\n",
    "## 7.4 FileDownloader Options [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1a280",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_downloader/options.html`**](https://onetl.readthedocs.io/en/stable/file/file_downloader/options.html)\n",
    "\n",
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.file import FileDownloader\n",
    "\n",
    "options = FileDownloader.Options(\n",
    "    if_exists=\"replace_entire_directory\",\n",
    "    delete_source=True,\n",
    "    workers=4,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792f54c",
   "metadata": {},
   "source": [
    "![](data/onetl_040.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca99ca3",
   "metadata": {},
   "source": [
    "<a id='part_75'></a>\n",
    "## 7.5 Методы FileDownloader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60464071",
   "metadata": {},
   "source": [
    "Метода всего два\n",
    "\n",
    "- `view_files()`\n",
    "- `run()`\n",
    "\n",
    "\n",
    "### `.view_files()`\n",
    "\n",
    "Возвращает список файлов который планируется обработать\n",
    "\n",
    "```python\n",
    "from onetl.file import FileDownloader\n",
    "downloader = FileDownloader(source_path=\"/remote\", ...)\n",
    "downloader.view_files()\n",
    "\n",
    "# Вывод на экран:\n",
    "\n",
    "# FileSet([\n",
    "#     RemoteFile(\"/remote/file1.txt\"),\n",
    "#     RemoteFile(\"/remote/file3.txt\"),\n",
    "#     RemoteFile(\"/remote/nested/file3.txt\"),\n",
    "# ])\n",
    "\n",
    "```\n",
    "\n",
    "### `.run()`\n",
    "\n",
    "В метод должны быть переданы абсолютные пути к файлам\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.file import FileDownloader\n",
    "\n",
    "downloader = FileDownloader(local_path=\"/local\", ...)\n",
    "\n",
    "downloaded_files = downloader.run(\n",
    "    [\n",
    "        \"/remote/filw1.txt\",\n",
    "        \"/any/nested/path/file2.txt\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "downloaded_files.successful = {\n",
    "    LocalPath(\"/local/file1.txt\"),\n",
    "    LocalPath(\"/local/file2.txt\"),\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea3da3",
   "metadata": {},
   "source": [
    "<a id='part_76'></a>\n",
    "## 7.6 FileDownloader Result [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6ba52",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_downloader/result.html`**](https://onetl.readthedocs.io/en/stable/file/file_downloader/result.html)\n",
    "\n",
    "Метод `.run()` возвращает объект `FileDownloader Result` у которого есть собственные атрибуты, свойства и методы\n",
    "\n",
    "- Атрибуты и свойства\n",
    "- Методы\n",
    "\n",
    "\n",
    "### FileDownloader Result категории свойств\n",
    "\n",
    "- `общие`\n",
    "- `failed`\n",
    "- `missing`\n",
    "- `skipped`\n",
    "- `successful`\n",
    "\n",
    "**FileDownloader Result - Общие свойства**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65422c",
   "metadata": {},
   "source": [
    "![](data/onetl_041.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e093776",
   "metadata": {},
   "source": [
    "**FileDownloader Result - Свойства по статусам файлов**\n",
    "\n",
    "![](data/onetl_042.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a77620",
   "metadata": {},
   "source": [
    "### Методы FileDownloader Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e5398",
   "metadata": {},
   "source": [
    "![](data/onetl_043.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3aecf",
   "metadata": {},
   "source": [
    "<a id='part_77'></a>\n",
    "## 7.7 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c95f8",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from  onetl.log import setup_logging\n",
    "from onetl.connection import FTP, S3\n",
    "from onetl.file import FileDownloader\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "ftp = FTP(\n",
    "    host=\"ftp\",\n",
    "    user=os.environ[\"FTP_USER\"],\n",
    "    password=os.environ[\"FTP_PASS\"],\n",
    "    port=21\n",
    ")\n",
    "\n",
    "ftp.check()\n",
    "ftp.list_dir(\"/\")\n",
    "ftp.list_dir(\"/test_dir\")\n",
    "\n",
    "# Эти параметры означают, что после скачивания, файлы в источнике будут стерты\n",
    "# А в получатели перезаписаны\n",
    "fd_options = FileDownloader.Options(\n",
    "    delete_source=True, \n",
    "    if_exists=\"replace_file\"\n",
    ")\n",
    "\n",
    "# Создадим экземпляр объекта скачивания файлов\n",
    "\n",
    "fd = FileDownloader(\n",
    "    connection=ftp,\n",
    "    local_path=\"./fd_test\",\n",
    "    options=of_options\n",
    ")\n",
    "\n",
    "fd.run([\"/test_dir/country_ref_from_ftp.csv\"])\n",
    "ftp.list_dir(\"/test_dir\")\n",
    "ftp.close()\n",
    "\n",
    "\n",
    "# Теперь время S3\n",
    "\n",
    "s3 = S3(\n",
    "    host=\"minio\",\n",
    "    protocol=\"http\",\n",
    "    port=9000,\n",
    "    bucket=os.environ[\"MINIO_MY_BUCKET\"],\n",
    "    access_key=os.environ[\"MINIO_ACCESS_KEY\"],\n",
    "    secret_key=os.environ[\"MINIO_SECRET_KEY\"],\n",
    ")\n",
    "\n",
    "\n",
    "s3.check()\n",
    "s3.list_dir(\"/\")\n",
    "s3.list_dir(\"/hive_at_s3.db\")\n",
    "s3.list_dir(\"/hive_at_s3.db/hive_test\")\n",
    "s3.list_dir(\"/hive_at_s3.db/hive_test/year=2012\")\n",
    "\n",
    "# Вывод\n",
    "# [RemoteFile('part-00000-1d1660d-b9ec-46fe-a0e5-5ec63499.c000.snappy.orc')]\n",
    "\n",
    "# импортируем из библиотеки метод работы с регулярными выражениями\n",
    "from onetl.file.filter import Regexp\n",
    "\n",
    "# Настроим фильтр так, чтобы забирать только файлы с расширением .orc \n",
    "\n",
    "fd = FileDownloader(\n",
    "    connection=s3,\n",
    "    local_path=\"./fd_test\",\n",
    "    source_path=\"/hive_at_s3.db/hive_test\",\n",
    "    filters=[Regexp(\".*.orc\")]\n",
    ")\n",
    "\n",
    "dr = fd.run()\n",
    "\n",
    "# Вывод словаря содержащего список скаченных файлов\n",
    "dr.dict()\n",
    "\n",
    "# Краткий результат\n",
    "dr.summary\n",
    "\n",
    "# Подробный результат\n",
    "dr.details\n",
    "\n",
    "# Список успешно скаченных\n",
    "dr.successrul\n",
    "\n",
    "s3.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fa876",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90771711",
   "metadata": {},
   "source": [
    "<a id='part_08'></a>\n",
    "# 8. Объект загрузки данных FileUploader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3d19a",
   "metadata": {},
   "source": [
    "<a id='part_81'></a>\n",
    "## 8.1 Объекты манипуляции данными [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708aee37",
   "metadata": {},
   "source": [
    "Объекты манипуляции данными позволяют производить массовые операции над файлами.\n",
    "\n",
    "**Основные:**\n",
    "\n",
    "- `FileDownloader`\n",
    "- `FileUploader`\n",
    "- `FileMover`\n",
    "\n",
    "**Вспомогательные**\n",
    "\n",
    "- `FileFilters` - для отбора файлов по определенному критерию\n",
    "- `FileLimits` - Критерий для огарничения количество обрабатываемых файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2a7e1",
   "metadata": {},
   "source": [
    "<a id='part_82'></a>\n",
    "## 8.2 FileUploader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39067696",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_uploader/`**](https://onetl.readthedocs.io/en/stable/file/file_uploader/)\n",
    "\n",
    "### Основные элементы конструкции\n",
    "\n",
    "- `FileUploader()`\n",
    "- `File Uploader Options`\n",
    "- `File Uploader Results`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228adf3e",
   "metadata": {},
   "source": [
    "<a id='part_83'></a>\n",
    "## 8.3 Конструктор FileUploader() [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2227f3",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_uploader/file_uploader.html`**](https://onetl.readthedocs.io/en/stable/file/file_uploader/file_uploader.html)\n",
    "\n",
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.connection import HDFS\n",
    "from onetl.file import FileUploader\n",
    "\n",
    "hdfs = HDFS(...)\n",
    "\n",
    "uploader = FileUploader(\n",
    "    connection=hdfs,\n",
    "    target_path=\"/path/to/remote/source\",\n",
    "    temp_path=\"/user/onetl\",\n",
    "    local_path=\"/some/local/directory\",\n",
    "    options=FileUploader.Options(delete_local=True,\n",
    "                                if_exists=\"replace_file\"),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e82e71",
   "metadata": {},
   "source": [
    "![](data/onetl_044.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b429ef",
   "metadata": {},
   "source": [
    "<a id='part_84'></a>\n",
    "## 8.4 FileUploader Options [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95eb7e",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_uploader/options.html`**](https://onetl.readthedocs.io/en/stable/file/file_uploader/options.html)\n",
    "\n",
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.file import FileUploader\n",
    "\n",
    "options = FileUploader.Options(\n",
    "    if_exists=\"replace_entire_directory\",\n",
    "    delete_local=True,\n",
    "    workers=4,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2f823",
   "metadata": {},
   "source": [
    "![](data/onetl_045.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61720778",
   "metadata": {},
   "source": [
    "<a id='part_85'></a>\n",
    "## 8.5 Методы FileUploader [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429dac9c",
   "metadata": {},
   "source": [
    "Метода всего два\n",
    "\n",
    "- `view_files()`\n",
    "- `run()`\n",
    "\n",
    "\n",
    "### `.view_files()`\n",
    "\n",
    "Возвращает список файлов который планируется обработать\n",
    "\n",
    "```python\n",
    "\n",
    "from onetl.file import FileUploader\n",
    "uploader = FileUploader(local_path=\"/local\", target_path=\"/remote\", ...)\n",
    "uploader.view_files()\n",
    "\n",
    "# Вывод на экран:\n",
    "\n",
    "# FileSet([\n",
    "#     LocalPath(\"/local/file1.txt\"),\n",
    "#     LocalPath(\"/local/file3.txt\"),\n",
    "#     LocalPath(\"/local/nested/file3.txt\"),\n",
    "# ])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### `.run()`\n",
    "\n",
    "В метод должны быть переданы абсолютные пути к файлам. \n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.file import FileUploader\n",
    "uploader = FileUploader(local_path=\"/local\", target_path=\"/remote\", ...)\n",
    "upload_result = uploader.run(\n",
    "    [\n",
    "        \"/local/file1\",\n",
    "        \"/local/nested/path/file3\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "upload_result\n",
    "\n",
    "# Вывод на экран:\n",
    "\n",
    "# UploadResult(\n",
    "#     successful=FileSet([\n",
    "#         RemoteFile(\"/remote/file1\"),\n",
    "#         RemoteFile(\"/remote/file2\"),\n",
    "#         # directory structure is preserved\n",
    "#         RemoteFile(\"/remote/nested/path/file3\")\n",
    "#     ]),\n",
    "#     failed=FileSet([FailedLocalFile(\"/local/failed.file\"),]),\n",
    "#     skipped=FileSet([LocalPath(\"/local/already.exists\"),]),\n",
    "#     missing=FileSet([LocalPath(\"/local/missing.file\"),]),\n",
    "# )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170a36b",
   "metadata": {},
   "source": [
    "<a id='part_86'></a>\n",
    "## 8.6 FileUploader Result [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e428b",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_uploader/result.html`**](https://onetl.readthedocs.io/en/stable/file/file_uploader/result.html)\n",
    "\n",
    "Метод `.run()` возвращает объект FileUploader Result у которого есть собственные атрибуты, свойства и методы\n",
    "\n",
    "- Атрибуты и свойства\n",
    "- Методы\n",
    "\n",
    "### FileUploader Result категории свойств\n",
    "\n",
    "- `общие`\n",
    "- `failed`\n",
    "- `missing`\n",
    "- `skipped`\n",
    "- `successful`\n",
    "\n",
    "**FileUploader Result - Общие свойства**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e1eb1",
   "metadata": {},
   "source": [
    "![](data/onetl_046.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491db37e",
   "metadata": {},
   "source": [
    "**FileUploader Result - Свойства по статусам файлов**\n",
    "\n",
    "![](data/onetl_047.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22480047",
   "metadata": {},
   "source": [
    "### Методы FileUploaderResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddbcb6",
   "metadata": {},
   "source": [
    "![](data/onetl_048.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0af8e2",
   "metadata": {},
   "source": [
    "<a id='part_87'></a>\n",
    "## 8.7 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211dfa0",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from onetl.log import setup_logging\n",
    "from onetl.connection import HDFS, Samba\n",
    "from onetl.file import FileUploader\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "hdfs = HDFS(host=\"namenode\", port=9870)\n",
    "hdfs.check()\n",
    "hdfs.list_dir(\"/\")\n",
    "hdfs.list_dir(\"/user/hadoop\")\n",
    "hdfs.create_dir(\"/user/hadoop/test_uploader\")\n",
    "hdfs.list_dir(\"/user/hadoop/test_uploader\")\n",
    "\n",
    "uploader = FileUploader(\n",
    "    connection=hdfs,\n",
    "    target_path=\"/user/hadoop/test_uploader\",\n",
    "    local_path=\"/opt/endata/test_folder\",\n",
    "    options=FileUploader.Options(delete_local=False,\n",
    "                                if_exists=\"replace_file\"),\n",
    ")\n",
    "\n",
    "uploader_result = uploader.run()\n",
    "\n",
    "uploader_result\n",
    "uploader_result.sumary\n",
    "\n",
    "hdfs.list_dir(\"/user/hadoop/test_uploader\")\n",
    "\n",
    "samba = Samba(\n",
    "    host=\"samba\",\n",
    "    share=os.environ[\"SAMBA_SHARE\"],\n",
    "    protocol=\"SMB\",\n",
    "    port=445,\n",
    "    user=os.environ[\"SAMBA_USER\"],\n",
    "    password=os.environ[\"SAMBA_PASS\"],\n",
    ")\n",
    "\n",
    "samba.list_dir(\"/\")\n",
    "\n",
    "uploader = FileUploader(\n",
    "    connection=samba,\n",
    "    target_path=\"/\",\n",
    "    local_path=\"/opt/endata/test_folder\",\n",
    "    options=FileUploader.Options(delete_local=False,\n",
    "                                if_exists=\"replace_file\")\n",
    ")\n",
    "\n",
    "uploader_result = uploader.run()\n",
    "uploader_result.details\n",
    "uploader_result.dict()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cbeaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e63b3",
   "metadata": {},
   "source": [
    "<a id='part_09'></a>\n",
    "# 9. FileMover, File Filters & File Limits [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ceeba",
   "metadata": {},
   "source": [
    "<a id='part_91'></a>\n",
    "## 9.1 Объекты манипуляции данными [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e13d0",
   "metadata": {},
   "source": [
    "Объекты манипуляции данными позволяют производить массовые операции над файлами.\n",
    "\n",
    "**Основные:**\n",
    "\n",
    "- `FileDownloader`\n",
    "- `FileUploader`\n",
    "- `FileMover`\n",
    "\n",
    "**Вспомогательные**\n",
    "\n",
    "- `FileFilters` - для отбора файлов по определенному критерию\n",
    "- `FileLimits` - Критерий для огарничения количество обрабатываемых файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a06646",
   "metadata": {},
   "source": [
    "<a id='part_92'></a>\n",
    "## 9.2 FileMover [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71724f",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_mover/`**](https://onetl.readthedocs.io/en/stable/file/file_mover/)\n",
    "\n",
    "Позволяет перемещать файлы внутри указанной фаловой системы, не используя скачивания фалов на локальную ФС.\n",
    "\n",
    "### Основные элементы конструкции\n",
    "\n",
    "- `FileMover()`\n",
    "- `FileMover Options`\n",
    "- `FileMover Results`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8928e",
   "metadata": {},
   "source": [
    "<a id='part_93'></a>\n",
    "## 9.3 Конструктор FileMover() [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65132743",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_mover/file_mover.html`**](https://onetl.readthedocs.io/en/stable/file/file_mover/file_mover.html)\n",
    "\n",
    "\n",
    "Пример:\n",
    "\n",
    "```python\n",
    "from onetl.connection import SFTP\n",
    "from onetl.file import FileMover\n",
    "\n",
    "sftp = SFTP(...)\n",
    "\n",
    "# create mover\n",
    "mover = FileMover(\n",
    "    connection=sftp,\n",
    "    source_path=\"/path/to/source/dir\",\n",
    "    target_path=\"/path/to/target/dir\",\n",
    ")\n",
    "\n",
    "# move files from \"/path/to/source/dir\" to \"/path/to/target/dir\"\n",
    "mover.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738c808",
   "metadata": {},
   "source": [
    "![](data/onetl_049.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18783eab",
   "metadata": {},
   "source": [
    "<a id='part_94'></a>\n",
    "## 9.4 FileMover Options [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05574de0",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_mover/options.html`**](https://onetl.readthedocs.io/en/stable/file/file_mover/options.html)\n",
    "\n",
    "Пример кода:\n",
    "\n",
    "```python\n",
    "from onetl.file import FileMover\n",
    "\n",
    "options = FileMover.Options(\n",
    "    if_exists=\"replace_entire_directory\",\n",
    "    workers=4,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42568ca",
   "metadata": {},
   "source": [
    "![](data/onetl_050.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225a877",
   "metadata": {},
   "source": [
    "<a id='part_95'></a>\n",
    "## 9.5 Методы FileMover [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ab713",
   "metadata": {},
   "source": [
    "Метода всего два\n",
    "\n",
    "- `view_files()`\n",
    "- `run()`\n",
    "\n",
    "\n",
    "### `.view_files()`\n",
    "\n",
    "Возвращает список файлов который планируется обработать\n",
    "\n",
    "```python\n",
    "from onetl.file import FileMover\n",
    "mover = FileMover(source_path=\"/remote\", ...)\n",
    "mover.view_files()\n",
    "\n",
    "# Вывод\n",
    "\n",
    "# FileSet([\n",
    "#     RemoteFile(\"/remote/file1.txt\"),\n",
    "#     RemoteFile(\"/remote/file2.txt\"),\n",
    "#     RemoteFile(\"/remote/nested/path/file3.txt\"),\n",
    "# ])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### `.run()`\n",
    "\n",
    "В метод должны быть переданы абсолютные пути к файлам. В метод run надо передатвать список, даже если в нем один элемент.\n",
    "\n",
    "\n",
    "```python\n",
    "from onetl.file import FileMover\n",
    "mover = FileMover(source_path=\"/source\", target_path=\"/target\", ...)\n",
    "move_result = mover.run()\n",
    "move_result\n",
    "\n",
    "# Вывод\n",
    "\n",
    "# MoveResult(\n",
    "#     successful=FileSet([\n",
    "#         RemoteFile(\"/target/file1.txt\"),\n",
    "#         RemoteFile(\"/target/file2.txt\"),\n",
    "#         # directory structure is preserved\n",
    "#         RemoteFile(\"/target/nested/path/file3.txt\"),\n",
    "#     ]),\n",
    "#     failed=FileSet([\n",
    "#         FailedRemoteFile(\"/source/failed.file\"),\n",
    "#     ]),\n",
    "#     skipped=FileSet([\n",
    "#         RemoteFile(\"/source/already.exists\"),\n",
    "#     ]),\n",
    "#     missing=FileSet([\n",
    "#         RemotePath(\"/source/missing.file\"),\n",
    "#     ]),\n",
    "# )\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b66c4",
   "metadata": {},
   "source": [
    "<a id='part_96'></a>\n",
    "## 9.6 FileMover Result [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13017e7b",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_mover/result.html`**](https://onetl.readthedocs.io/en/stable/file/file_mover/result.html)\n",
    "\n",
    "Метод `.run()` возвращает объект FileMover Result у которого есть собственные атрибуты, свойства и методы\n",
    "\n",
    "- Атрибуты и свойства\n",
    "- Методы\n",
    "\n",
    "### FileMover Result категории свойств\n",
    "\n",
    "- `общие`\n",
    "- `failed`\n",
    "- `missing`\n",
    "- `skipped`\n",
    "- `successful`\n",
    "\n",
    "**FileMover Result - Общие свойства**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e1a90",
   "metadata": {},
   "source": [
    "![](data/onetl_051.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d6a64",
   "metadata": {},
   "source": [
    "**FileMover Result - Свойства по статусам файлов**\n",
    "\n",
    "![](data/onetl_052.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b813e3",
   "metadata": {},
   "source": [
    "### Методы FileMoverResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd604f2",
   "metadata": {},
   "source": [
    "![](data/onetl_053.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486afad",
   "metadata": {},
   "source": [
    "<a id='part_97'></a>\n",
    "## 9.7 FileFilters [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c6396",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_filters/index.html`**](https://onetl.readthedocs.io/en/stable/file/file_filters/index.html)\n",
    "\n",
    "Объект `FileFilters` применяется для отбора файлов для обрботки в объектах `FileDownloader` и `FileMover`\n",
    "\n",
    "Задать ограничения можно используя одну из трех опций:\n",
    "\n",
    "- `Glob` - выражения Glob\n",
    "- `Regexp` - регулярные выражения\n",
    "- `ExcludeDir` - путь к директории файлы которой следует исключить во время обработки\n",
    "\n",
    "Одновременно может применяться несколько фильтров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4aaf97",
   "metadata": {},
   "source": [
    "<a id='part_98'></a>\n",
    "## 9.8 FileLimits [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb8ebc",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/file/file_limits/index.html`**](https://onetl.readthedocs.io/en/stable/file/file_limits/index.html)\n",
    "\n",
    "Объект `FileLimits` применяется только в объектах `FileDownloader` и `FileMover` и позволяет ограничить общее количество \n",
    "\n",
    "- `MaxFilesCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1a493",
   "metadata": {},
   "source": [
    "<a id='part_99'></a>\n",
    "## 9.9 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c46c1d",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from onetl.log import setup_logging\n",
    "from onetl.connection import S3\n",
    "from onetl.file import FileMover\n",
    "from onetl.file.filter import Glob, Regexp\n",
    "from onetl.file.limit import MaxFilesCount\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "s3 = S3(\n",
    "    host=\"minio\",\n",
    "    protocol=\"http\",\n",
    "    port=9000,\n",
    "    bucket=\"test-files\",\n",
    "    access_key=os.environ[\"MINIO_ACCESS_KEY\"],\n",
    "    secret_key=os.environ[\"MINIO_SECRET_KEY\"]\n",
    ")\n",
    "\n",
    "s3.check()\n",
    "\n",
    "# Создадим фильтр, который будет забирать только CSV файлы\n",
    "\n",
    "filter = Glob(\"*.csv\")\n",
    "\n",
    "# Создадим ограничения для количества обрабатываемых файлов. Будем работать только с одним\n",
    "\n",
    "limit = MaxFilesCount(1)\n",
    "\n",
    "# Создадим экземпляр объекта FileMover\n",
    "\n",
    "mover = FileMover(\n",
    "    connection=s3,\n",
    "    source_path=\"/old\",\n",
    "    target_path=\"/new\",\n",
    "    filters=[filter],\n",
    "    limits=[limit]\n",
    ")\n",
    "\n",
    "mover.run()\n",
    "\n",
    "# Создадим новый фильтр. Он будет сложнее\n",
    "# Он будет отбирать файлы excel в имени которых будет слово \"horizon\"\n",
    "\n",
    "filter = [Glob(\"*.xlsx\"), Regexp(\"horizon\")]\n",
    "\n",
    "mover = FileMover(\n",
    "    connection=s3,\n",
    "    source_path=\"/old\",\n",
    "    target_path=\"/new\",\n",
    "    filters=filter\n",
    ")\n",
    "\n",
    "mover.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd7d41",
   "metadata": {},
   "source": [
    "<a id='part_010'></a>\n",
    "# 10. HWM & HWM Store. Snapshot strategy & Incremental strategy. Batch strategies [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42507e3",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/hwm_store/`**](https://onetl.readthedocs.io/en/stable/hwm_store/)\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/strategy/index.html`**](https://onetl.readthedocs.io/en/stable/strategy/index.html)\n",
    "\n",
    "Список стратегий чтения:\n",
    "\n",
    "- `YAMLHWMStore()`\n",
    "- `SnapshotStrategy`\n",
    "- `IncrementalStrategy`\n",
    "- `SnapshotBatchStrategy`\n",
    "- `IncrementalBatchStrategy`\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3ea25",
   "metadata": {},
   "source": [
    "<a id='part_101'></a>\n",
    "## 10.1 HWM - High Water Mark [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6026e6",
   "metadata": {},
   "source": [
    "**HWM** - `High Water Mark` - Это метки которые используются для инкрементальных процессов обработки данных. \n",
    "\n",
    "В нашем случае это прежде всего метки чтения данных. Когда мы работаем с большими объемами данных мы не всегда можем обработать их в один проход, мы даже не всегда можем прочитать их за один раз. \n",
    "\n",
    "Когда это происходит нам нужен инструмент, который позволит нам помнить что мы уже обработали. В случае наших инструментов это очень простая вещь - метка - запись до которой мы прочитали данные. Такой меткой может быть индентификатор, timestamp, или суррогатный ключ, который мы можем использовать чтобы отделить уже обработанные записи, от тех которые еще требуют обработки. \n",
    "\n",
    "И если с его вычислением в момент обработки все относительно просто, то хранение его в моменты между итерациями процесса могут вызывать сложности. \n",
    "\n",
    "Да в небольших локальных процессах нет ничего сложного, чтобы сохранить значения куда то. Например в локальный файл. Но когда мы работает в распределенной среде и обрабатываем данные действительно большого размера, а процессы могут выполнятся на разных хостах разными командами задача становится не такой элементарной, вот для ее решения мы и предоставляем ряд инструментов. Они позволяют нам сохранить отметку о прочитанных данных и получить ее при следующем чтении, чтобы отобрать только нужные нам записи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e8675",
   "metadata": {},
   "source": [
    "![](data/onetl_054.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7b2ec",
   "metadata": {},
   "source": [
    "<a id='part_102'></a>\n",
    "## 10.2 HWMStore [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ff2e3",
   "metadata": {},
   "source": [
    "Чтобы хранить метки HWM нам прежде всего нужен объект который предоставляет нам хранилище и его интерфейс. Для первых попыток, простых процессов, onETL предоставляет `YAMLHWMStore`\n",
    "\n",
    "[**`https://onetl.readthedocs.io/en/stable/hwm_store/yaml_hwm_store.html`**](https://onetl.readthedocs.io/en/stable/hwm_store/yaml_hwm_store.html)\n",
    "\n",
    "Связь с более продвинутыми HWM store обеспечивается библиотекой `etl-entities`, она предоставляет базовые объекты для фиксирования HWM, объекты для типов значений HWM, базовые объекты подключения к HWM.\n",
    "\n",
    "[**`https://etl-entities.readthedocs.io/en/stable/`**](https://etl-entities.readthedocs.io/en/stable/)\n",
    "\n",
    "И для сложных процессов, было разработано приложение, хранилище HWM меток. Внутри компании развернут экземпляр этого приложения для коммунального использования.\n",
    "\n",
    "[**`https://horizon-hwm-store.readthedocs.io/en/stable/horizon-hwm-store.html`**](https://horizon-hwm-store.readthedocs.io/en/stable/horizon-hwm-store.html)\n",
    "\n",
    "- `YAMLHWMStore`\n",
    "- `Horizon через etl-entities`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88199a7f",
   "metadata": {},
   "source": [
    "<a id='part_103'></a>\n",
    "## 10.3 YAMLHWMStore [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fa9ce",
   "metadata": {},
   "source": [
    "Допустим мы решили использовать локальный файл для хранения наших HWM. Для того чтобы это сделать необходимо воспользоваться объектом `YAMLHWMStore`.\n",
    "\n",
    "Конструктор этого объекта принимает два параметра: путь к файлу, куда будут сохраняться HWM и кодировка.\n",
    "\n",
    "Объект предоставляет два метода: `get_hwm()` и `set_hwm()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820486f",
   "metadata": {},
   "source": [
    "![](data/onetl_055.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23d29",
   "metadata": {},
   "source": [
    "### Пример использования  `YAMLHWMStore`\n",
    "\n",
    "Тут видно нашего локального хранилища HWM с помощью контекстного менеджера `YAMLHWMStore`.\n",
    "\n",
    "Поскольку мы не передавили в него никакого параметра, будет использоваться путь по умолчанию. Каким он будет, зависит от вашего окружения.\n",
    "\n",
    "```python\n",
    "from onetl.connection import Hive, Postgres\n",
    "from onetl.db import DBReader, DBWriter\n",
    "from onetl.strategy import IncrementalStrategy\n",
    "from onetl.hwm.store import YAMLHWMStore\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "maven_packages = Postgres.get_packages()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"some-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "postgres = Postgres(\n",
    "    host=\"postgres.domain.com\",\n",
    "    user=\"myself\",\n",
    "    password=\"****\",\n",
    "    database=\"target_database\",\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=postgres,\n",
    "    source=\"public.mydata\",\n",
    "    columns=[\"id\", \"data\"],\n",
    "    hwm=DBReader.AutoDetectHWM(name=\"some_unique_name\", expression=\"id\"),\n",
    ")\n",
    "\n",
    "writer = DBWriter(connection=hive, target=\"db.newtable\")\n",
    "\n",
    "with YAMLHWMStore():\n",
    "    with IncrementalStrategy():\n",
    "        df = reader.run()\n",
    "        writer.run(df)\n",
    "\n",
    "# will create file\n",
    "# \"~/.local/share/onETL/id__public.mydata__postgres_postgres.domain.com_5432__myprocess__myhostname.yml\"\n",
    "# with encoding=\"utf-8\" and save a serialized HWM values to this file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e085f",
   "metadata": {},
   "source": [
    "<a id='part_104'></a>\n",
    "## 10.4 Стратегии чтения [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37907f7",
   "metadata": {},
   "source": [
    "Стратегии чтения, они именно про чтение, то есть результатом являются данные прочитанные в датафрейм, следущие операции полностью определяет пользователь.\n",
    "\n",
    "- **`Snapshot`** - данные прочитываются целиком в датафрейм\n",
    "- **`SnapshotBatch`** - Данные прочитываются целиком, но разбиваются на фрагменты (пакеты) размер которых определен пользователем\n",
    "- **`Incremental`** - Прочитыватся только та часть данных которая полявилась, после предыдущего чтения, после которого было сохранено значение HWM\n",
    "- **`IncrementalBatch`** - Прочитываются только новые данные и при чтении разбиваются на фрагменты, которые определил пользователь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e2e56",
   "metadata": {},
   "source": [
    "![](data/onetl_056.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a0410",
   "metadata": {},
   "source": [
    "<a id='part_105'></a>\n",
    "## 10.5 Стратегия Snapshot [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddf1cb",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/strategy/snapshot_strategy.html`**](https://onetl.readthedocs.io/en/stable/strategy/snapshot_strategy.html)\n",
    "\n",
    "- `YAMLHWMStore`\n",
    "- `Horizon через etl-entities`\n",
    "\n",
    "В случае применения стратегии `Snapshot` в датафрейм будут прочитаны все данные из источника. Эта стратегия не поддерживает HWM.\n",
    "\n",
    "Эта стратегия извлекает все записи из источника.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de907802",
   "metadata": {},
   "source": [
    "![](data/onetl_057.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5831df7",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import SFTP\n",
    "from onetl.file import FileDownloader\n",
    "from onetl.strategy import SnapshotStrategy\n",
    "\n",
    "sftp = SFTP(\n",
    "    host=\"sftp.domain.com\",\n",
    "    user=\"user\",\n",
    "    password=\"*****\",\n",
    ")\n",
    "\n",
    "downloader = FileDownloader(\n",
    "    connection=sftp,\n",
    "    source_path=\"/remote\",\n",
    "    local_path=\"/local\",\n",
    ")\n",
    "\n",
    "with SnapshotStrategy():\n",
    "    df = downloader.run()\n",
    "\n",
    "# current run will download all files from 'source_path'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a8dba",
   "metadata": {},
   "source": [
    "<a id='part_106'></a>\n",
    "## 10.6 Стратегия `Snapshot Batch` [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5643f00",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/strategy/snapshot_batch_strategy.html`**](https://onetl.readthedocs.io/en/stable/strategy/snapshot_batch_strategy.html)\n",
    "\n",
    "Применяя эту стратегию, мы извлекаем нужные нам данные последовательными операциями чтения. Ее удобно применять когда таблица источник у нас большого размера или забирать данные за одну итерацию неудобно по тем или инным причинам. Но при этом нам всетаки необходимо извлечь все данные в рамках одного процесса.\n",
    "\n",
    "У этой стратегии есть параметры:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fdd92",
   "metadata": {},
   "source": [
    "![](data/onetl_058.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8772266",
   "metadata": {},
   "source": [
    "В качестве поля значения hwm мы используем столбец с идентификатором и ограничиваем размер пакета сотней записей. \n",
    "\n",
    "Здесь так же используется `.AutoDetectHWM()` объекта `DBReader`в него мы передаем имя нашего hwm. Имя по которому мы будем в дальнейшем находить информацию об извлеченных записях и имя столбца параметра `expression`. \n",
    "\n",
    "Здесь так же могут быть использованы вычисляемые значения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d4a4a",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Hive, Postgres\n",
    "from onetl.db import DBReader, DBWriter\n",
    "from onetl.strategy import SnapshotBatchStrategy\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "maven_packages = Postgres.get_packages()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"some-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "postgres = Postgres(\n",
    "    host=\"postgres.domain.com\",\n",
    "    user=\"myself\",\n",
    "    password=\"****\",\n",
    "    database=\"target_database\",\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=postgres,\n",
    "    source=\"public.mydata\",\n",
    "    columns=[\"id\", \"data\"],\n",
    "    hwm=DBReader.AutoDetectHWM(name=\"some_hwm_name\", expression=\"id\"),\n",
    ")\n",
    "\n",
    "writer = DBWriter(connection=hive, target=\"db.newtable\")\n",
    "\n",
    "with SnapshotBatchStrategy(step=100) as batches:\n",
    "    for _ in batches:\n",
    "        df = reader.run()\n",
    "        writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f599e",
   "metadata": {},
   "source": [
    "<a id='part_107'></a>\n",
    "## 10.7 Стратегия `Incremental` [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3db66e",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/strategy/incremental_strategy.html`**](https://onetl.readthedocs.io/en/stable/strategy/incremental_strategy.html)\n",
    "\n",
    "Эта стратегия применяется когда мы разными процессами прочитываем данные из источника. Ее следует применять для регулярных загрузок данных из постоянно обновляемых источников. \n",
    "\n",
    "Если первый две стратегии предполагают, что нам каждый раз необходимо прочитать полностью данные, то здесь мы работаем только с теми, которые еще не читали.\n",
    "\n",
    "Эта стратегия принимает параметр `offset`, то есть мы можем начать загрузку, отступив какое то количество значений, от начального значения нашей инкрементальной метки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6697",
   "metadata": {},
   "source": [
    "![](data/onetl_059.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f27bf",
   "metadata": {},
   "source": [
    "Каждый последующий запуск, будет проверять наличие в хранилище HWM инкрементальной метки с именем `\"some_hwm_name\"` в случае ее наличия применять для ограничения извлекаемых записей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ad58d",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Postgres, Hive\n",
    "from onetl.db import DBReader, DBWriter\n",
    "from onetl.strategy import IncrementalStrategy\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "maven_packages = Postgres.get_packages()\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"some-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "postgres = Postgres(\n",
    "    host=\"postgres.domain.com\",\n",
    "    user=\"myself\",\n",
    "    password=\"****\",\n",
    "    database=\"target_database\",\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=postgres,\n",
    "    source=\"public.mydata\",\n",
    "    columns=[\"id\", \"data\"],\n",
    "    hwm=DBReader.AutoDetectHWM(name=\"some_hwm_name\", expression=\"id\"),\n",
    ")\n",
    "\n",
    "writer = DBWriter(connection=hive, target=\"db.newtable\")\n",
    "\n",
    "with IncrementalStrategy():\n",
    "    df = reader.run()\n",
    "    writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee4d2f",
   "metadata": {},
   "source": [
    "<a id='part_108'></a>\n",
    "## 10.8 Стратегия `IncrementalBatch` [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787dac1d",
   "metadata": {},
   "source": [
    "[**`https://onetl.readthedocs.io/en/stable/strategy/incremental_batch_strategy.html`**](https://onetl.readthedocs.io/en/stable/strategy/incremental_batch_strategy.html)\n",
    "\n",
    "Эта стратегия применяется в тех случаях, когда размер инкремента достаточно велик, чтобы нам было неудобно или неприемлемо обрабатывать его целиком.\n",
    "\n",
    "То есть в рамках этой стратегии будет проверено, извлекались ли ранее данные. И если извлекались, они будут разбиты на части в соответствиии с передаными нами параметрами.\n",
    "\n",
    "Параметры точно такие же как в случае батчевой загрузки `Snapshot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedab3d",
   "metadata": {},
   "source": [
    "![](data/onetl_060.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553b8b9",
   "metadata": {},
   "source": [
    "```python\n",
    "from onetl.connection import Postgres, Hive\n",
    "from onetl.db import DBReader, DBWriter\n",
    "from onetl.strategy import IncrementalBatchStrategy\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "maven_packages = Postgres.get_packages()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"some-name\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(maven_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "postgres = Postgres(\n",
    "    host=\"postgres.domain.com\",\n",
    "    user=\"myself\",\n",
    "    password=\"****\",\n",
    "    database=\"target_database\",\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "hive = Hive(cluster=\"rnd-dwh\", spark=spark)\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=postgres,\n",
    "    source=\"public.mydata\",\n",
    "    columns=[\"id\", \"data\"],\n",
    "    hwm=DBReader.AutoDetectHWM(name=\"some_hwm_name\", expression=\"id\"),\n",
    ")\n",
    "\n",
    "writer = DBWriter(connection=hive, target=\"db.newtable\")\n",
    "\n",
    "with IncrementalBatchStrategy(step=100) as batches:\n",
    "    for _ in batches:\n",
    "        df = reader.run()\n",
    "        writer.run(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da7193",
   "metadata": {},
   "source": [
    "<a id='part_109'></a>\n",
    "## 10.9 Демо [▴](#head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56e1bc",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import os\n",
    "from onetl.log import setup_logging\n",
    "from onetl.connection import S3, MongoDB\n",
    "from onetl.file.filter import Glob\n",
    "from onetl.file import FileDownloader\n",
    "from onetl.strategy import IncrementalStrategy\n",
    "from onetl.hwm.store import YAMLHWMStore # объект доступа к локальному хранилищу HWM\n",
    "from etl_entities.hwm import FileListHWM # объект HWM дл списка файлов\n",
    "from pyspark.sql import SparkSession\n",
    "from onetl.db import DBReader\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType\n",
    ")\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "s3 = S3(\n",
    "    host=\"minio\",\n",
    "    protocol=\"\"http,\n",
    "    port=9000,\n",
    "    bucket=\"test-files\",\n",
    "    access_key=os.environ[\"MINIO_ACCESS_KEY\"],\n",
    "    secret_key=os.environ[\"MINIO_SECRET_KEY\"],\n",
    ")\n",
    "\n",
    "s3.check()\n",
    "\n",
    "filter = Glob('*.csv')\n",
    "\n",
    "downloader = FileDownloader(\n",
    "    connection=s3,\n",
    "    source_path=\"/old\",\n",
    "    local_path=os.environ[\"FILES_FOLDER\"],\n",
    "    hwm=FileListHWM(name=\"my_files_hwm\"),\n",
    "    filters=[filter]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af938fbf",
   "metadata": {},
   "source": [
    "Проверим что папки для файлов пусты\n",
    "\n",
    "![](data/onetl_061.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36358dca",
   "metadata": {},
   "source": [
    "```python\n",
    "# Используя контекстный менеджер для работы с HWM скачаем файлы с S3\n",
    "\n",
    "with YAMLHWMStore(path=\"/opt/endata/hwm_store\"):\n",
    "    with IncrementalStrategy():\n",
    "        downloader.run()      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18962d",
   "metadata": {},
   "source": [
    "Проверим что файлы, которые мы забрали с S3 появились в папке назначения\n",
    "\n",
    "![](data/onetl_062.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870c879",
   "metadata": {},
   "source": [
    "Теперь посмотрим, что изменилось в нашем локальном хранилище HWM\n",
    "\n",
    "![](data/onetl_063.png)\n",
    "\n",
    "Появились данные о том, что мы получили 3 файла"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e395197",
   "metadata": {},
   "source": [
    "```python\n",
    "# Попробуем скачать файлы еще раз\n",
    "\n",
    "with YAMLHWMStore(path=\"/opt/endata/hwm_store\"):\n",
    "    with IncrementalStrategy():\n",
    "        downloader.run()   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4876d",
   "metadata": {},
   "source": [
    "Во время выполнения, мы получаем сообщение о том, что нет файлов для скачивания.\n",
    "\n",
    "![](data/onetl_064.png)\n",
    "\n",
    "То есть использование контекстного менеджера позволило совместить несколько операций:\n",
    "1. Проверить какие файлы мы уже скачали\n",
    "2. Сопоставить с этим списком файлы на источнике\n",
    "3. И выбрать из них такие, которые мы еще не скачивали\n",
    "\n",
    "В данном случае их нет, по этому мы ничего и не скачали."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e0b46",
   "metadata": {},
   "source": [
    "```python\n",
    "# Теперь поработаем с MongoDB\n",
    "\n",
    "packages = MongoDB.get_packages(spark_version=\"3.4.2\")\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName(\"test\") \\\n",
    "                    .config(\"spark.jars.packages\", \".\".join(packages)) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "mongo = MongoDB(\n",
    "    host=\"mongodb\",\n",
    "    user=os.environ[\"MONGODB_USERNAME\"],\n",
    "    password=os.environ[\"MONGODB_PASSWORD\"],\n",
    "    database=os.environ[\"MONGODB_DATABASE\"],\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "mongo.check()\n",
    "\n",
    "df_schema = StructType(\n",
    "    [\n",
    "        StructField(\"country_id\", IntegerType()),\n",
    "        StructField(\"country_code\", StringType()),\n",
    "        StructField(\"country_name\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Создадим объект чтения из базы данных используя коннектор MongoDB\n",
    "# созданную схему данных\n",
    "# И указав в качестве значения HWM идентификатор поле \"Страна\" \n",
    "\n",
    "\n",
    "reader = DBReader(\n",
    "    connection=mongo,\n",
    "    table=\"country_ids\",\n",
    "    df_schema=df_schema,\n",
    "    hwm=DBReader.AutoDetectHWM(name=\"my_mongo_hwm\",\n",
    "                              expression=\"country_id\")\n",
    ")\n",
    "\n",
    "# Используя контекстный менеджер локального хранилища HWM меток и инкрементальную стратегию. Прочитаем данные из MongoDB\n",
    "\n",
    "with YAMLHWMStore(path=\"/opt/endata/hwm_store\"):\n",
    "    with IncrementalStrategy():\n",
    "        mongo_df = reader.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a18e14",
   "metadata": {},
   "source": [
    "В логах выполнения операции мы видим значение, которое должно быть записано в HWM  \n",
    "\n",
    "![](data/onetl_065.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca2448",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что у нас есть в локальном хранилище HWM\n",
    "\n",
    "![](data/onetl_066.png)\n",
    "\n",
    "Мы видим что файлов с HWM стало два. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91a1e8",
   "metadata": {},
   "source": [
    "Давайте посмотрим что записано в файле с HWM для наших операций с MongoDB\n",
    "\n",
    "![](data/onetl_067.png)\n",
    "\n",
    "Мы видим те же данные, которые были в логе выполнения при операции чтения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0454be",
   "metadata": {},
   "source": [
    "```python\n",
    "# Остановим сессию спарк\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
